---
title: "Classification Models for New Enrolled Students"
author: "Frank Laudert"
date: "2022-12-19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Import R Libraries


```{r}




library(rmarkdown)

library(reticulate)

library(knitr)



library(dplyr)

library(ggplot2)

library(tidyr)

library(pander)

library(kableExtra)

library(forcats)

library(skimr)

library(gt)

library(GGally)

library(gmodels)

library(plotly)

library(scales)


library(ggforce)

library(ggdist)

library(lubridate)

library(DT)

library(visdat)



library(packcircles)

library(viridis)

library(ggthemes)

library(purrr)

library(ggtext)

library(RColorBrewer)

library(ggpubr)
```



Import Python Libraries

```{python}




import pandas as pd

import numpy as np

import datetime


from dateutil.relativedelta import relativedelta

from datetime import date
```




# Introduction

<br/><br/>

<br/><br/>

This analysis will review different classification models on their
ability to predict new student persistence, i.e. do first time
undergraduate students entering for the fall term continue enrollment in
the spring term of the same academic year. Only students enrolled in
Bachelor degrees will be used for this analysis.

This report is designed so sections of interest can be accessed directly
from the index on the left. Code for this project is included for
interest sake but its understanding is not required for interpretation
of the visualizations and tables produced. The purpose of this project
is for any individual to understand the results of the models without
delving into how the models are built and fit.

R version 4.1.3 and Python version 3.8.12 are used for this for this
project.


```{r}

ug_v1<-readRDS("ug_v1")



```


# Initial Data Analysis

<br/><br/>

This section will check the structure and completeness of the data. In
other words we will check how many columns and rows the data set has and
if there are any missing values. Columns are the features (variables)
and rows are the observations.

<br/><br/>





```{r}


dim(ug_v1)v


```






<br/><br/>

From the above output we find that the data set has 23 columns and 2,667
rows.

<br/><br/>

<br/><br/>








```{r}


class_str<-vis_dat(ug_v1)

```







<br/><br/>

*Figure 1* visually presents the breakdown of variable types indicated
by color.

From this plot it is determined that there are 15 variables of type
*character,* 6 variables of type numeric and 2 variables of type date
(POSIXct). The color gray provides an overview of the depth of missing
(NULL) values for each variable.

<br/><br/>







```{r}



  
  
  mis_val<-vis_miss(ug_v1, cluster=TRUE,sort_miss = TRUE)


```







<br/><br/>

*Figure 2* provides an in depth analysis of missing (NULL) values for
the data set as a whole and for each individual variable. First, the
plot gives the total percentage of missing values in the entire data set
(3.7%). The plot presents more detail of missing values by providing
information on a per variable basis. The extent of missing values within
a specific variable is visually represented by black shaded areas.
Located at the top of the plot, beside each variable name, is the
percentage of missing values for that variable. *English Your Primary
Language* and Stage_dte features have by far the largest percentage
missing data at 31.9% and 26.43% respectively.

<br/><br/>

# Cleaning and Preparation

<br/><br/>

Cleaning and preparation involves removing duplicate rows/columns,
filling missing entries with statistical methods , dropping or combining
categories, renaming variables or categories, removing null entries, and
creating new variables.

<br/><br/>

## Categorical Features

<br/><br/>

From Figure 1 we know that there are 15 categorical variables in the
new_ug data set that are of type *`character`*. Categorical variables
must be of type *factor* for use in classification models, thus these
variables will be transformed to type *`factor`*.

<br/><br/>

<!--# mutate_if function from dplyr package will be useed to transform character variables to type factor -->

<br/><br/>



```{r}


ug_v2<-ug_v1 %>% 
   mutate_if(is.character, factor)



```








```{r}


class_str_2<-vis_dat(ug_v2)


```


Figure 2a shows all *categorical* variables colored in orange are now
type *factor*.

<br/><br/>







```{r}


v2_sum_1<-ug_v2 %>% 
  select_if(is.factor) %>% 
  summary()

```









```{r}

pander(v2_sum_1, "Table 1: new_ug_v2 Summary Factor Variables")



```







<br/><br/>

From *Table 1* it can be viewed that certain categorical variables have
categories that exhibit value inconsistency (representations of the same
category). These value inconsistencies are due to differences in
capitalization or different abbreviations. These categories will be
re-coded so as to eliminate value inconsistencies.

<br/><br/>

Categories for the *filed_fafsa* and *enrolled_spring* variables will be
re-coded so all are upper case.



<!--# The recode() function from dplyr package will be used to  change the values of factor levels -->





```{r}


ug_v3<- ug_v2 %>%
  mutate(filed_fafsa=recode(filed_fafsa, 'y' ="Y" )) %>% 
  mutate(enrolled_spring=recode(enrolled_spring, 'y' ="Y" )) %>% 
  mutate(english_your_primary_language=recode(english_your_primary_language, 'y' ="Y", "u"="unk", "Unk"="unk" )) 



```









```{r}


incon_var<-ug_v3 %>% 
  select(filed_fafsa, english_your_primary_language, 
         enrolled_spring) %>% 
  summary()


```








```{r}

pander(incon_var, "Table 2:  Summary Recoded Variables ")


```

<br/><br/>







Certain variables have unnecessary categories and thus need to be
reduced. This will be handled by collapsing smaller categories into
general and bigger categories.

The *value_description* variable has a number of categories with low
counts and as such these categories will be collapsed into others. All
low count categories will be collapsed into a a general category
"*other",* except for"*Hispanic/Latino"* , which will be collapsed into
"*Hispanic"*. Also, the name *value_description will be renamed to
race.*

<br/><br/>









```{r}


 ug_v3<-ug_v3 %>% 
  mutate(value_description=recode(value_description,"Race and Ethnicity unknown"="unkown" , "Two or more races"="other", 
     "American Indian or Alaska Native" ="other",
     "Native Hawaiian or Other Pacific Islander" = "other","Nonresident Alien"="other", "Hispanic/Latino"="Hispanics of any race"))


```








```{r}


ug_v3<-ug_v3 %>% 
  rename("race"="value_description")


```



```{r}


ug_v3<-ug_v3 %>% 
  mutate(race=recode(race,"Hispanics of any race"="Hispanic" , "Black or African American"="African American"))

```








```{r}

val_des_cnt_2<-ug_v3 %>%
  count(race,sort=TRUE, name="Total Count") 

```



```{r}


pander(val_des_cnt_2, caption= "Table 3: Race  Variable Count-Recoded")


```






```{r}


deg_cde_cnt<-ug_v3 %>%
  count(degr_cde,sort=TRUE, name="Total Count") 




```








```{r}

pander(deg_cde_cnt, caption= "Table 4: Degree Code Variable Count")


```




As seen from *Table 4* above, the *degree code* feature has a number of
categories that will be eliminated due to not being a bachelor degree.
As such, the *degree code* categories *CAGS, CT, AS*, and *PRECT* will
be omitted from the data set.


```{r}


ug_v3<-ug_v3 %>%  
  filter(degr_cde %in% c("BS", "BA", "BSMG", "BAMS", "BSHS", "BAPSY"))


```



Now that un-needed categories have been eliminated from ***degree
code***, the remaining categories will be collapsed into either *BS* or
*BA* to simplify analysis and visualization.

<br/><br/>


```{r}

ug_v3<- ug_v3 %>%
   mutate(degr_cde=recode(degr_cde, 'BSMG' ="BS", "BAMS"="BA", "BSHS"="BS","BAPSY"="BA"))

```





```{r}


degr_cde_cnt_2<-ug_v3 %>%
  count(degr_cde,sort=TRUE, name="Total Count") 


```



```{r}



ug_v3$degr_cde<-droplevels(ug_v3$degr_cde)
ug_v3$race<-droplevels(ug_v3$race)
```


```{r}



pander(degr_cde_cnt_2, caption= "Table 5: Degree Code Variable (Combined) count")


```


```{r}


prog_desc_cnt<-ug_v3 %>%
  count(prog_desc,sort=TRUE, name="Total Count") 
  
  

```


```{r}



 pander(prog_desc_cnt,caption= "Table 6: Program Description Count")
 
 
```



<br/><br/>

Categories in the *Program Description* feature will be shortened for
presentation purposes.

<br/><br/>

```{r}


ug_v3<- ug_v3 %>%
   mutate(prog_desc=recode(prog_desc, 'Multidisciplinary Studies' ="Multidisc Studies", "Early Childhood Education and Care"="Early Childhood Educ/Care",
           "Health Care Management"="Health Care Mgmt",
         "Bachelor of Science in Business Administration"="Business Admin",
         "Wellness and Health Promotion"="Wellness & Health Promotion",
         "Bachelor of Science in Healthcare Administration" = "Healthcare Admin", "Bachelor of Science in Accounting"="Accounting","Human Services Management"="Human Services Mgmt","Natural and Applied Sciences"="Natural & Applied Sciences",
         "Bachelor of Science in Quality Syst & Imprv Mgmt"="Quality Syst & Imprv Mgmt","Bachelor of Science in Quality Systems Management"="Quality Systems Mgmt","BA in Multidisciplinary Studies (CCG)"="Multidisc Studies", 
         "Bachelor of Science in Digital Marketing"="Digital Marketing",
         "Information Technology Management"="Information Technology Mgmt",
         "Management Studies"="Mgmt Studies"))


```



```{r}


prog_desc_cnt_2<-ug_v3 %>%
  count(prog_desc,sort=TRUE, name="Total Count")
```



```{r}


pander(prog_desc_cnt_2, "Table 7: Program Description Count (Category Names Shortened")
```


## Numeric Variables

```{r}


num_sum<-ug_v3 %>% 
  select_if(is.numeric) %>% 
  summary()
```


```{r}

pander(num_sum,caption= "Table 8 Numeric Variables Summary")


```



<br/><br/>

*Table 8* shows the feature *hrs_enrolled* has a minimum of zero which
indicates a student did not enroll for the fall term. Since the analysis
is based on students who have enrolled at the college the *hrs_enrolled*
feature will be filtered to determine how many observations have zero
for *hrs_enrolled*.

<br/><br/>



```{r}

ug_v3 %>% select(hrs_enrolled) %>% summary()


```




<br/><br/>

Any observation that has *hrs_enrolled* as zero will be removed.

<br/><br/>




```{r}



ug_v4<-ug_v3 %>% 
  filter(hrs_enrolled > 0)



```








```{r}



ug_v4 %>% 
  select(hrs_enrolled) %>% 
  filter(hrs_enrolled==0)


```



## Feature Creation

<br/><br/>

New features will be created to enhance the analysis and modeling.

<br/><br/>






<br/><br/>

*Python* will be used for the creation of certain features. In order for
*Python* to be used on a R data frame within *the R Markdown*
environment, the *R* data.frame must be converted to a *Pandas* data
frame.

<br/><br/>


<!--# New features will be created to enhance the analysis and modeling.   -->

<!--#   Python will be used for the creation of certain features. In order for Python to be used on a R data frame within the R Markdown environment, the R data.frame must be converted to a Pandas data frame.   -->




```{python}


new_enroll_p=r.ug_v4


```




```{python}



new_enroll_p.info()
```







<br/><br/>

<!--# The feature stageDT_classStart_diff will be created from the difference between the date a student was accepted (hist_stage_dte) and the class start date. The time difference is reported in days.  -->

<br/><br/>





```{python}



new_enroll_p["accpt_Start_diff"]=(new_enroll_p["stage_dte"]-new_enroll_p["class_st_dte"]).dt.days




```







```{python}


new_enroll_p["accpt_Start_diff"].head()

```






```{python}



new_enroll_p["accpt_Start_diff"].describe()


```





<br/><br/>

<!--#  The feature age will be created by calculating the difference between the date of the term a student started and that student's date of birth. This new feature is the age of the student at the start of their first fall term of enrollment. -->






```{python}



new_enroll_p["age"]=(new_enroll_p["class_st_dte"]-new_enroll_p["birth_dte"])

new_enroll_p["age"]=new_enroll_p["age"]/np.timedelta64(1,'Y')



```







```{python}


new_enroll_p[["age"]].info()




```







```{python}



new_enroll_p["age"].head()

```







<br/><br/>

<!--#  The above output shows age has six decimal points. The will be rounded up to two decimal points.  -->

<br/><br/>






```{python}








new_enroll_p["age"]=new_enroll_p["age"].round()



```








```{python}


new_enroll_p["age"].head()



```





<br/><br/>

<!--#  The feature percent of enrolled credits earned by dividing credits earned by credits attempted. -->








```{python}



new_enroll_p["pct_enrolled_cr_earned"]=(new_enroll_p["trm_hrs_earned"]/new_enroll_p["hrs_enrolled"])


```






```{python}


new_enroll_p["pct_enrolled_cr_earned"].head()


```







```{python}



new_enroll_p["pct_enrolled_cr_earned"].describe()




```








<br/><br/>

<!--#   The Pandas data frame will be converted to a R data.frame   -->

<br/><br/>


```{python}



r.ug_v5=new_enroll_p



```






<!--# We will switch back to R chunks -->




<br/><br/>

A new categorical variable will be created from the numeric variable
*hrs_enrolled*.

<br/><br/>



```{r}


ug_v8<-ug_v8 %>% 
  mutate(term_enrolled_sts=case_when(
    between(hrs_enrolled, 1,5) ~ "less than half time",
    between(hrs_enrolled, 6,8) ~ "half time",
    between(hrs_enrolled, 9,11)~ "three quarter time",
    TRUE ~ "full time"))


```





```{r}


ug_v8$term_enrolled_sts<-as.factor(ug_v8$term_enrolled_sts)



```







```{r}



levels(ug_v8$term_enrolled_sts)



```



<br/><br/>

<!--#   The new categorical variable term_hrs_sts should be ordinal yet the output of the levels show"three quarter time" is located in the fourth slot. Its correct slot should be 2nd behind the "full time" category. The categories will be re-leveled to remedy this.   -->

<br/><br/>






<!--# The fct_relevel() function from the forcats package will be used change the levels  -->


```{r}


ug_v8<-ug_v8%>% 
  mutate(term_enrolled_sts=fct_relevel(term_enrolled_sts,"three quarter time", after=1))



```



```{r}



ug_v8$prog_cde<-droplevels(ug_v8$prog_cde)



```







<br/><br/>

<!--#  Year of class start variable will be created by extracting year from the class_st_dte variable.    -->


<br/><br/>






```{r}


ug_v8<-ug_v8 %>% 
  mutate(year=lubridate::year(class_st_dte))



```







```{r}



ug_v9<-ug_v8 %>% 
  filter(race %in% c("Hispanics of any race", "Black or African American", "White", "Asian", "other"))



```








<br/><br/>

# Exploratory Data Analysis

<br/><br/>

<br/><br/>











<!--# Create count data frame for loc_perc plot  -->



```{r}




gender_years<-ug_v9 %>%
  count(year, gender) %>%
  #mutate(perc = round(n / sum(n),2)) %>% 
  complete(gender,year, fill=list(n=0))
  

cumulative_gender<-gender_years %>% 
  split(f=.$year) %>% 
  accumulate(.,~bind_rows(.x, .y)) %>% 
  bind_rows(.id="frame")




```





```{r}



gend_years_plt<-cumulative_gender %>% 
  plot_ly(x=~year, y=~n, color=~gender) %>% 
  add_lines(frame=~frame, ids=~gender)



```





```{r echo=FALSE,fig.cap="Figure 3: Enrollment by gender(2010-2021)"", message=FALSE, warning=FALSE,fig.height=6, fig.width=6 }



gend_years_plt

```






```{r}


gen_per<-ug_v9 %>% 
  filter(year >2015) %>% 
  group_by(year) %>% 
  count(gender) %>% 
  mutate(perc = round(n / sum(n),2)) 



```






```{r}





gen_plt<-ggplot(gen_per, aes(x = reorder(gender, -perc), y = perc, fill=gender))+ 
  geom_bar(stat = "identity")+
  ggtitle("Enrollment by Gender (2016-2021) ")+
  labs( fill = "Gender")+
  scale_y_continuous(labels=scales::percent_format())+
  geom_text(aes(label=percent(perc),
                y=perc + .01),
            position=position_dodge(0.2),
            vjust=.05)+
  theme(axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        plot.title=element_text(size=10, hjust=0.5))+
  facet_wrap(~year)


```





```{r echo=FALSE, fig.cap="Figure 4: Percentage of Enrollment by gender(2016-2021)", fig.height=4, fig.width=4, message=FALSE, warning=FALSE}



gen_plt



```






<br/><br/>

*Figure 3* shows enrollment split by gender over time from 2010 to 2021.
The trend for male student appears to decline relative to total
enrollment after 2015. **Figure 4** concentrates on the years on the
years 2016 to 2021. From the bar plots we can see that male enrollment
as a percentage of total enrollment decreased by 6 to 7 percent starting
at 2018 and every year after.

<br/><br/>








```{r}


race_years<-ug_v9 %>% 
  count(year, race) %>% 
  complete(race, year, fill=list(n=0))

cumulative_race<-race_years %>% 
  split(f=.$year) %>% 
  accumulate(.,~bind_rows(.x, .y)) %>% 
  bind_rows(.id="frame")



```







```{r}





ethn_time_plt<-cumulative_race %>% 
  plot_ly(x=~year, y=~n, color=~race) %>% 
  add_lines(frame=~frame, ids=~race)



```





```{r echo=FALSE,fig.cap="Figure 5 fig.height=5, fig.width=6, message=FALSE, warning=FALSE}




ethn_time_plt





```









```{r}



eth_per<-ug_v9 %>% 
  filter(year >2015) %>% 
  group_by(year) %>% 
  count(race) %>% 
  mutate(perc = round(n / sum(n),2)) 



```







```{r}




eth_plt<-ggplot(eth_per, aes(x = reorder(race, -perc), y = perc, fill=race))+ 
  geom_bar(stat = "identity")+
  ggtitle("Enrollment by Race (2016-2021) ")+
  labs(x = "Race", y = "Percent", fill = "Race")+
  scale_y_continuous(labels=scales::percent_format())+
  theme(
    axis.ticks.x=element_blank(),
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.x=element_blank(),
        axis.text.y=element_text(size=7),
        plot.title=element_text(size=10, hjust=0.5),
    legend.title=element_text(size=10))+
  facet_wrap(~year)



```






```{r echo=FALSE, fig.cap="Figure 6", fig.height=4, fig.width=4, message=FALSE, warning=FALSE}



eth_plt


```






<br/><br/>

*Figure 5* shows enrollment split by race over time from 2010 to 2021.
From this plot we can see enrollment for African American and Hispanic
students dropped more dramatically than other groups. *Figure 6* shows
enrollment of Hispanic students as a percentage of total enrollment
declining at 2019 and dropping below thirty percent in 2020 and 2021.
African American enrollment as a percentage of total enrollment declined
below thirty strarting at 2017 and for every year to 2021.

<br/><br/>








```{r}




fafsa_rac_plt<-ug_v9 %>% 
  group_by(race,year) %>% 
  count(filed_fafsa) %>% 
  mutate(perc = round(n / sum(n),2)) %>% 
  ggplot(aes(x=reorder(x=race, -perc),y=perc, fill=filed_fafsa))+ 
  geom_bar(stat="identity")+
   ggtitle("Financial Aid Filing Status by Race and Year")+
  labs(x="Race", fill="Filed FAFSA")+
  scale_y_continuous(labels=scales::percent_format())+
  theme(axis.ticks.x=element_blank())+
  theme(axis.text.x=element_text(angle=70, hjust=1),
        axis.text.y=element_text(size=7),
        axis.title.y=element_blank(),
        axis.title.x=element_blank(),
        legend.title=element_text(size=9),
        legend.text=element_text(size=6),
        plot.title=element_text(size=10, hjust=0.5))+
  facet_wrap(vars(year))



```














```{r echo=FALSE, fig.cap="Figure 7", fig.height=5, fig.width=5}




fafsa_rac_plt



```







<br/><br/>



```{r}



prog_desc_sub<-ug_v9 %>% 
  count(prog_desc) %>% 
  mutate(perc = round(n / sum(n),2)) 





```








```{r}




prog_desc_perc_2<-ggplot(prog_desc_sub, aes(x =reorder(prog_desc), y = perc, group=1, text=paste("Program Description: ",prog_desc,"<br>Percent: ", perc)))+ 
  geom_point(
    stat="identity",size=3,color="orange")+
  ggtitle("Program Enrollment")+
  labs(x = "Program Description", y = "Percent")+
  scale_y_continuous(labels=scales::percent_format())+
  theme(axis.text.x=element_text(angle=90, hjust=1),
        axis.title.x=element_blank(),
    axis.title.y=element_blank(),
    plot.title=element_text(size=10, hjust=0.5))+
  coord_flip()




```










```{r echo=FALSE, fig.cap="Figure 8", fig.height=8, fig.width=8, message=FALSE, warning=FALSE}




ggplotly(prog_desc_perc_2,tooltip = "text")

```










```{r}




prog_desc_plot <- ggplot(ug_v9, aes(y=prog_desc, x=gender, color=gender,group=prog_desc)) +
  geom_count(alpha=0.5) +
  labs(title = "Programs Split by Gender",
       size = "14")+
   theme(
    axis.title.x=element_blank(),
    axis.title.y=element_blank(),
    plot.title=element_text(size=9, hjust=0.5),
    axis.text.x=element_blank()
  )



```










```{r echo=FALSE, fig.cap="Figure 9"}



ggplotly(prog_desc_plot)


```











```{r}



prog_top<-ug_v9 %>% 
  filter(prog_desc %in% c("Mgmt Studies", "Human Services", "Multidisc Studies",
                          "Psychology", "Early Childhood Educ/Care", 
                          "Health Care Mgmt")) %>% 
  group_by(year) %>% 
  count(prog_desc)



```











```{r}


prog_line_plt<-prog_top %>% 
  ggplot(aes(x=year, y=n, group=prog_desc, color=prog_desc, text=paste("Program Description: ",prog_desc,"<br>Count: ", n)))+
  ggtitle("Top Six Program Enrollment 2010-2021")+
  labs( color = "Program Description")+
  theme(
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.x=element_text(size=6),
         axis.text.y=element_text(size=8),
        legend.text=element_text(size=6),
        legend.title=element_text(size=7),
        plot.title=element_text(size=10, hjust=0.5))+
  geom_line(alpha=0.4)




```












```{r echo=FALSE, fig.cap="Figure 9", fig.height=8, fig.width=6, message=FALSE, warning=FALSE}



ggplotly(prog_line_plt,tooltip = "text")




```









<br/><br/>

*Figure 8* displays program enrollment as a percentage of total
enrollment over twelve years. From this plot it is clear that Management
Studies is the largest program with almost one fourth of the total
enrollment. Figure 9 breaks the program down by gender. We can see that
Management Studies has large enrollments for both men and women. After
Management studies it appears women enrollment is diversified among
other programs comparpred to men. Figure 10 shows enrollment trend over
time of the six largest enrolled programs. Though all programs declined
in enrollment, Management Studies and Multidisciplinary studies had
dramatic declines in enrollment.

<br/><br/>











<br/><br/>











```{r}




gender_enroll_sts<-ug_v9 %>% 
   count(term_enrolled_sts, gender) %>%
  #mutate(prop=n/sum(n)) %>%
 # group_by(term_enrolled_sts) %>%
  plot_ly(x=~term_enrolled_sts, y=~n,color=~gender,hoverinfo="y") %>% 
  add_bars() %>% 
  layout(title = 'Term Enrollment Status Split by Gender', xaxis = list(title = 'Term Enrollment Status'), font=t, plot_bgcolor = "#e5ecf6",

         yaxis = list(title = 'Count'), legend = list(title=list(text='Gender', barmode='group')))




```










```{r echo=FALSE, fig.cap="Figure 10",fig.height=7, fig.width=6, message=FALSE, warning=FALSE}


gender_enroll_sts




```









```{r}



enr_sts_age_gender<-ug_v9 %>% 
  ggplot(aes(x=term_enrolled_sts, y=age, fill=term_enrolled_sts))+
  geom_boxplot()+
  facet_wrap(~gender)+
   labs(x = "Enrollment Status", y = "Age")+
  theme(legend.position = "none")






```






```{r echo=FALSE, fig.cap="Figure 11", fig.height=7, fig.width=6, message=FALSE, warning=FALSE}



enr_sts_age_gender



```















<br/><br/>



<br/><br/>








```{r}


p_2<-ggplot(ug_v9, aes(x=age, y=gender)) + 
  geom_jitter(position=position_jitter(0.2))+
  ggtitle("Strip Chart Age by Gender")




```







```{r}


pt_2<-p_2 + coord_flip()

pa_2<-pt_2 + stat_summary(fun=median, geom="point", shape=18,
                 size=3, color="red")

pa_pt_2<-pa_2+scale_color_grey() + theme_classic()



```








```{r echo=FALSE,  fig.cap="Figure 13: Strip Chart-Age by Gender", message=FALSE, warning=FALSE}



pa_pt_2


```












<br/><br/>

*Figure 11* shows that over half of all female students were enrolled
half-time (6-8 credits) while males were more evenly distributed with
half-time enrollment at 40% and full-time at 33%. *Figure 12* displays
goes further by adding the age feature to the equation via a box plot.
For the purpose of this analysis there are only two parts of the box
plot we need two focus on. The line within the box is the median. Fifty
percent of all age values are above the line and fifty percent of all
age values are below the line. The length of the box includes fifty
percent of population for that specific box plot. For both females and
males the median age increases as enrolled credits decrease. The only
difference between men and women is the half time box plot. Fifty
percent of women for Half time enrollment status fall between ages 26
and 49 whereas fifty percent of men fall between ages 27 and 40. This
may be important later on when the classification models are evaluated.
*Figure 13* displays age distribution of gender in a simplified form.
From this plot we can observe that both men and women are heavily
concentrated under the age of thirty. Women distribution of age thin out
between forty and fifty compared to men whose age distribution thins out
right after thirty.

<br/><br/>








```{r}



ug_stat<-ug_v9

ug_stat$term_enrolled_sts<-factor(ug_stat$term_enrolled_sts,                                    # Change ordering manually
                  levels = c("full time", "three quarter time", 
                             "half time", "less than half time"))



```









```{r}


enroll_year_plt<-ug_v9 %>% 
  group_by(term_enrolled_sts,year) %>% 
  count(gender) %>% 
  mutate(perc = round(n / sum(n),2)) %>% 
  ggplot(aes(x=reorder(x=term_enrolled_sts, -perc),y=perc, fill=gender))+ 
  geom_bar(stat="identity")+
   ggtitle("Enrollment Status by Gender and Year")+
  labs(x="Gender", fill="Enrollment Stats")+
  scale_y_continuous(labels=scales::percent_format())+
  theme(axis.ticks.x=element_blank())+
  theme(axis.text.x=element_text(angle=70, hjust=1),
        axis.text.y=element_text(size=7),
        axis.title.y=element_blank(),
        axis.title.x=element_blank(),
        legend.title=element_text(size=8),
        legend.text=element_text(size=6),
        plot.title=element_text(size=10, hjust=0.5))+
  facet_wrap(vars(year))


```










```{r echo=FALSE, fig.cap="Figure 14"}




enroll_year_plt



```






<br/><br/>

Figure 14 views enrollment status by each year. Of interest is the full
time enrollment of men. For the years 2012 through 2017, full time
enrollment hovered between thirty and fifty percent. After 2017, full
time enrollment for men dropped below thirty percent.

<br/><br/>






```{r}



trns_cred_box<-ug_v9 %>% 
   filter(race %in% c("Hispanic","African American","White","Asian")) %>% 
  ggplot(aes(x=race, y=xfer_hrs_earned, fill=race))+
  geom_boxplot()+
  ggtitle("Transfer Credits by Race")+
  facet_wrap(vars(race))+
  theme(legend.position = "none",
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.x=element_blank(),
        plot.title=element_text(size=10, hjust=0.5))



```













```{r echo=FALSE, fig.cap="Figure 15"}


ggplotly(trns_cred_box)




```















```{r}




tran_cred<-ug_v9 %>% 
  select(race, xfer_hrs_earned, year) %>% 
  filter(race %in% c("Hispanic","African American","White","Asian")) %>% 
  group_by(race, year) %>% 
mutate(median_tr=median(xfer_hrs_earned),
       q1=quantile(xfer_hrs_earned, p=.25),
       q3=quantile(xfer_hrs_earned, p=.75))





```






```{r}




tr_cr_plt<-ggplot(tran_cred,aes(x=year, y=median_tr,group=1,text=paste("Year: ",year, "Median Transfer Credits: ",median_tr)))+
  geom_line(color="red")+
  geom_line(aes(x=year, y=q1))+
  geom_line(aes(x=year, y=q3))+
  geom_hline(yintercept=3)+
  #geom_ribbon(aes(ymin=q1, ymax=q3))+
  ggtitle("Transfer Credits by Year and Race")+
  facet_wrap(vars(race))+
    theme(
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.x=element_text(size=6),
         axis.text.y=element_text(size=7),
        plot.title=element_text(size=10, hjust=0.5))+
  facet_wrap(vars(race))


```







```{r echo=FALSE, echo=FALSE, fig.cap="Figure 16"}



ggplotly(tr_cr_plt)



```





br/><br/>

*Figure 15* displays transfer credits by race. In terms transfer credit
hours, the Asian student population has a median transfer credit amount
of 54 which is 20 more than the other student groups. Transfer credits
are of importance as more credits will shorten the time to degree
completion and may increase the amount of financial aid a student
receives. Figure 16 presents transfer credits by race over time. This
may help point out changes in by year.

<br/><br/>






```{r}




gpa_box<-ug_v9 %>% 
   filter(race %in% c("Hispanic","African American","White","Asian")) %>% 
  ggplot(aes(x=race, y=trm_gpa, fill=race))+
  geom_boxplot()+
  ggtitle("GPA by Race")+
  facet_wrap(vars(race))+
  theme(legend.position = "none",
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.x=element_blank(),
        plot.title=element_text(size=10, hjust=0.5))



```
















```{r echo=FALSE, fig.cap="Figure 17,fig.height=11, fig.width=11,  message=FALSE, warning=FALSE}


ggplotly(gpa_box)


```















```{r}





gpa_yr<-ug_v9 %>% 
  select(race, trm_gpa, year) %>% 
  filter(race %in% c("Hispanic","African American","White","Asian")) %>% 
  group_by(race, year) %>% 
mutate(median_gpa=median(trm_gpa))



```






```{r}





gpa_yr_plt<-ggplot(gpa_yr,aes(x=year, y=median_gpa,group=1,text=paste("Year: ",year, "Median GPA: ",median_gpa)))+
  geom_line(color="red")+
  #geom_ribbon(aes(ymin=q1, ymax=q3))+
  ggtitle("GPA by Year and Race")+
  facet_wrap(vars(race))+
    theme(
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.x=element_text(size=6),
         axis.text.y=element_text(size=7),
        plot.title=element_text(size=10, hjust=0.5))+
  facet_wrap(vars(race))


```














```{r echo=FALSE, fig.cap="Figure 18"}





ggplotly(gpa_yr_plt)


```








```{r}





eth_enroll<-ug_v9 %>% 
  filter(race %in% c("Hispanic","African American","White","Asian")) %>% 
  count(race, enrolled_spring) %>% 
  group_by(race) %>% 
  mutate(perc = n / sum(n)) %>% 
  ggplot(aes(x=race, y=perc, fill=enrolled_spring))+
  geom_col(position="dodge")+
  ggtitle("Spring Enrollment by Race")+
  labs(x = "Race", y = "Percent", fill = " Enrolled Spring")+
  scale_y_continuous(labels=scales::percent_format())+
  #geom_text(aes(label=percent(perc),
                #y=perc + .03),
           # position=position_dodge(.9),
            #vjust=.05)+
  theme(
    axis.ticks.x=element_blank(),
    axis.text.x=element_text(angle=70, hjust=1),
    axis.title.y=element_blank(),
    axis.title.x=element_blank(),
    plot.title=element_text(size=10, hjust=0.5),
    legend.title=element_text(size=9),
    legend.text=element_text(size=7))





```






```{r echo=FALSE, fig.cap="Figure 19"}



eth_enroll




```










```{r}




spr_race<-ug_v9 %>% 
  filter(race %in% c("Hispanic","African American","White","Asian")) %>% 
  group_by(race, year) %>% 
  count(enrolled_spring) %>% 
  mutate(perc = round(n / sum(n),2)) %>% 
  ggplot(aes(x=year, y=perc, color=enrolled_spring,
             group=1,text=paste("Year: ",year, "Percent: ",perc, "Enrolled Spring:", enrolled_spring)))+ 
  geom_line(stat="identity")+
  ggtitle("Spring Enrolled by Year and Race")+
  labs(color = " Enrolled Spring")+
   scale_y_continuous(labels=scales::percent_format())+
  theme(
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.x=element_text(size=6),
         axis.text.y=element_text(size=7),
        legend.text=element_text(size=6),
        legend.title=element_text(size=7),
        plot.title=element_text(size=10, hjust=0.5))+
  facet_wrap(vars(race))



```










```{r echo=FALSE, fig.cap="Figure 20"}


ggplotly(spr_race,tooltip = "text")


```











```{r}





corr_plot_2<-ug_v9 %>% 
  select(enrolled_spring,hrs_enrolled, trm_gpa, xfer_hrs_earned, 
    accpt_Start_diff,age, pct_enrolled_cr_earned) %>% 
  ggscatmat(color="enrolled_spring", 
            corMethod = "pearson",
            alpha=0.2)




```








```{r echo=FALSE, fig.cap="Figure 21"}


corr_plot_2



```





<br/><br/>

From *Figure 3* we find that there is only one relationship that has a
very high correlation, pct_enrolled_cr_earned and hrs_enrolled. The
correlation plot splits the relationship by the different outcomes of
the response variable: *enrolled_spring.* Outcome of *yes* has a high
correlation of .87 while an outcome of *No* has a very high correlation
of .92. All other correlations either either low or negligible.

<br/><br/>

<br/><br/>

<br/><br/>

The data set will be filtered to include only features that will be
included in the classification models.




```{r}




ug_v10<-ug_v9 %>% 
  select(enrolled_spring,term_enrolled_sts, race, accpt_Start_diff, age, xfer_hrs_earned, filed_fafsa, trm_gpa, pct_enrolled_cr_earned, gender, degr_cde) %>%  mutate_if(is.character, factor)



```








```{r}


 ug_v10<-ug_v10 %>% 
  filter(race %in% c("Asian", "African American",
                                  "Hispanic","White"))





```




```{r}



names(ug_v10)



```





```{r}


ug_v10$race<-droplevels(ug_v10$race)



```






```{r}


saveRDS(ug_v10, "ug_v10.rds")


```




```{r}


ug_v10<-readRDS("ug_v10")


```








<br/><br/>

# Models

<br/><br/>

## Pre-processing / feature engineering

<br/><br/>







```{r}


library(tidymodels)



library(juicr)


```




<br/><br/>

The data will be split into training and testing sets. <br/><br/>


```{r}


set.seed(20)
new_ug_split<-initial_split(ug_v10, prop=0.70,strata=enrolled_spring)


```







```{r}


new_ug_training<-training(new_ug_split)


```





```{r}


new_ug_test<-testing(new_ug_split)



```




<br/><br/>

75% of the data has been split into the training set and 25% into the
testing set.

<br/><br/>







```{r}


dim(new_ug_training)


dim(new_ug_test) 





```





<br/><br/>

The next step is to partition the training set into equal subsets. The
subsets will be used to assess a models performance on training data
through cross validation.

The process works by setting aside the first fold as a test set and the
remaining subsets are used as the aggregated training set. The model is
trained on the aggregated training set then the performance is evaluated
on the testing set. This will continue until all folds have been held
out as a test set. An evaluation metric is calculated for each iteration
then averaged together. This results in a cross validated metric.

<br/><br/>







```{r}


set.seed(100)

ug_folds<-vfold_cv(new_ug_training, v=5)


```











```{r}


ug_folds



```



```{r}


library(themis)


```




<!--#  The recipe function is used for performing feature engineering.    -->

<!--#  step_zv removes any feature that has a zero variance.   -->

<!--# step_log() will log transform data (since some of our numerical variables are right-skewed)    -->

<!--#  step_normalize converts numerical features to the same scale.  -->

<!--#  step_dummy transforms the categorical features into dummy variables.  -->

<!--#  step_smote up samples the target feature using nearest neighbor algorithm. -->












```{r}


new_rec<-recipe(enrolled_spring ~ ., data=new_ug_training) %>% 
  #step_zv(all_numeric()) %>% 
  step_log(age) %>% 
  step_normalize(all_numeric()) %>% 
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_smote(enrolled_spring) 


```






<br/><br/>

<!--# Check if all of the pre-processing steps from above actually worked,    -->

<br/><br/>








```{r}


prepped_data <- 
  new_rec %>% # use the recipe object
  prep() %>% # perform the recipe on training data
  juice() # extract only the preprocessed dataframe 



```








```{r}


new_rec %>%  prep() %>% bake(new_data=NULL) %>% count(enrolled_spring)


```






<!--#  The above output verifies the target feature enrolled_spring classes have the same number of observations.  -->





```{r}



glimpse(prepped_data)

```





<!--#  The glimpse function shows all numeric features have been scaled and all categorical features are now in binary form.  -->







```{r}



Prepped_corrplt<-prepped_data %>% 
  select(xfer_hrs_earned,
         age,
         accpt_Start_diff,
         ) %>% 
  ggscatmat(corMethod = "spearman",
            alpha=0.2)


```








```{r}



Prepped_corrplt


```







## Model Descriptions

<!--#   First the model will be built. This includes setting the specification. Certain model specifications will be determined by a tuning grid.  -->

<!--#   Once the specification is set the models will be fit to the validation set (cv_folds) and used to estimate the performance of each model using the fit_resamples() function.  -->

<!--#   fit_resamples() will fit a model to each re-sample and evaluate on the held out sample (test set). This repeats until all re-sample has been fit as the test set.  -->







<br/><br/>

### Decision Tree

Decision trees use a flowchart like a tree structure to show the
predictions that result from a series of feature splits. In order to
accomplish this, a decision tree is made up of three types of nodes:

-   Root Node (parent node): The node that starts the graph. It
    evaluates the variable that best splits the data.

-   Intermediate Nodes (child nodes): These are nodes where features are
    evaluated for further splits of the data but are not the final
    nodes.

-   Leaf nodes (terminal nodes): These are the final nodes of the tree,
    where the prediction of a categorical event are made.

For a more detailed explanation of decision trees check the link below.

[Guide to Decision
Trees](https://www.analyticsvidhya.com/blog/2021/08/decision-tree-algorithm/)








```{r}


library(rpart)



```





<!--#  Set the specifications for the decision tree model  -->







```{r}


doParallel::registerDoParallel()

tree_spec_untuned <- decision_tree(
  min_n=tune(),
tree_depth=tune(),
cost_complexity = tune()) %>% 
      set_engine('rpart') %>% 
      set_mode('classification')



```








```{r}



tree_grid<-grid_regular(extract_parameter_set_dials(tree_spec_untuned),levels=3)


```






```{r}



tree_tune_results<-tune_grid(
  tree_spec_untuned, 
  enrolled_spring~., 
  resamples=ug_folds
  #metrics=metric_set(accuracy, roc_auc)
)



```





```{r}


final_tree_params<-select_best(tree_tune_results, "roc_auc")



```






```{r}



final_tree_params



```








```{r}



ree_params_table<-ggtexttable(final_tree_params, rows = NULL, 
                        theme = ttheme("mBlue", base_size=11)) %>% 
   tab_add_title("Final Decision Tree Parameters",face="bold") 




```







```{r}



 tree_params_table


```







```{r}

tree_spec<-finalize_model(tree_spec_untuned, final_tree_params)



```





<br/><br/>

<!--#   The two previous steps will be combines using workflows. A workflow combines the pre-processing recipe and model specification.   -->

<!--#    By creating a workflow, all of the pre-processing will be handled during the fitting of the model and when generating new predictions. Workflows are equivalent to Python's Scikit Learn's pipeline.  -->






```{r}




tree_wf<-workflow() %>% 
  add_model(tree_spec) %>% 
  add_recipe(new_rec)




```




<!--#  Fit decision tree model to cross validation folds  -->






```{r}



doParallel::registerDoParallel()

tree_rs<-tree_wf %>% 
  fit_resamples(
    resamples=ug_folds,
    metrics=metric_set(roc_auc, accuracy, sensitivity, specificity, f_meas, precision),
    control=control_resamples(save_pred = TRUE)
  )




```






### Random Forest

Random Forest models are made up of individual decision trees whose
predictions are combined for a final result. The final result is decided
using majority rules which means that the final prediction is what the
majority of the decision tree models chose. Random Forests can be made
up of thousands of decision trees.

Simply put, the random forest builds multiple decision trees and merges
them together to get a more accurate prediction.

[Random Forest for
Beginners](https://www.analyticsvidhya.com/blog/2021/10/an-introduction-to-random-forest-algorithm-for-beginners/)

<!--#   Set the specifications for the random forest model  -->







```{r}

library(randomForest)



```






```{r}


rf_spec_untuned <- rand_forest(
 # mtry=tune(),
  trees=tune(),
  min_n=tune()) %>% 
      set_engine('randomForest') %>% 
      set_mode('classification')



```






```{r}



rf_grid<-grid_regular(extract_parameter_set_dials(rf_spec_untuned),levels=3)



```







```{r}




doParallel::registerDoParallel()

rf_tune_results<-tune_grid(
  rf_spec_untuned, 
  enrolled_spring~., 
  resamples=ug_folds
  #metrics=metric_set(accuracy, roc_auc)
)


```





```{r}



final_rf_params<-select_best(rf_tune_results, "roc_auc")



```







```{r}


final_rf_params



```








```{r}




rf_params_table<-ggtexttable(final_rf_params, rows = NULL, 
                        theme = ttheme("mGreen", base_size=11)) %>% 
   tab_add_title("Final Random Forest Parameters",face="bold")


```







```{r}


rf_params_table



```







```{r}




rf_spec<-finalize_model(rf_spec_untuned, final_rf_params)



```








```{r}






rf_wf<-workflow() %>% 
  add_model(rf_spec) %>% 
  add_recipe(new_rec)



```





<!--#  Fit the models to the re-sampled folds.  -->






```{r}





doParallel::registerDoParallel()

rf_rs<-rf_wf %>% 
  fit_resamples(
    resamples=ug_folds,
    metrics=metric_set(roc_auc, accuracy, sensitivity, specificity, f_meas, precision),
    control=control_resamples(save_pred = TRUE)
  )



```







### Logistic Regression

<br/><br/>

Logistic Regression is a binary classification model that finds the
probability or odds ratio of an event. For this model there are two
events or possible outcomes, yes or no. A probability between 0 and 1 of
an observation belonging to one of the two categories is produced.

If you would like to know more about Logistic Regression check out the
link below.

[Logistic Regression for
Beginners](https://www.analyticsvidhya.com/blog/2021/08/conceptual-understanding-of-logistic-regression-for-data-science-beginners/)

<!--#  Set the specifications for the logistic regression model  -->






```{r}




log_reg_spec<- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")



```








```{r}





logreg_wf<-workflow() %>% 
  add_model(log_reg_spec) %>% 
  add_recipe(new_rec)




```









```{r}





doParallel::registerDoParallel()

log_rs<-logreg_wf %>% 
  fit_resamples(
    resamples=ug_folds,
    metrics=metric_set(roc_auc, accuracy, sensitivity, specificity, f_meas, precision),
    control=control_resamples(save_pred = TRUE)
  )




```




### KNN

<br/><br/>

Nearest neighbor methods work by taking the value of an observation's
attribute (known also as features or variables) and then locating
another observation whose attribute value is numerically nearest to it.

More on the Nearest Neighbor algorithm can be found at the following
link.

[Introduction to KNN
Algorithms](https://www.analyticsvidhya.com/blog/2022/01/introduction-to-knn-algorithms/)






```{r}



knn_spec<-nearest_neighbor() %>% 
  set_engine("kknn") %>% 
  set_mode("classification")



```





```{r}



knn_wf<-workflow() %>% 
  add_model(knn_spec) %>% 
  add_recipe(new_rec)


```




```{r}

doParallel::registerDoParallel()

knn_rs<-knn_wf %>% 
  fit_resamples(
    resamples=ug_folds,
    metrics=metric_set(roc_auc, accuracy, sensitivity, specificity,f_meas, precision),
    control=control_resamples(save_pred = TRUE)
  )



```







### XGBoost

Gradient boosting ,just as random forest, is mad up of many individual
decision trees. The difference is instead of using the majority vote of
the combined decision trees it adjusts a decision tree model based on
the incorrect predictions from the decision tree before. New trees are
fit based entirely on the errors from the previous tree's predictions.
That is to say predictions that are accurate are not included in the fit
of new trees. New trees are added sequentially until no further
improvements can be made.

More information of the Xgboost algorithm can be found at the below
link.

[Guide to
XGBoost](https://www.analyticsvidhya.com/blog/2018/09/an-end-to-end-guide-to-understand-the-math-behind-xgboost/)










```{r}





xgb_spec<-boost_tree(trees=2000, mtry=5) %>% 
  set_mode("classification") %>% 
  set_engine("xgboost")




```







```{r}



xgb_wf<-workflow() %>% 
  add_model(xgb_spec) %>% 
  add_recipe(new_rec)




```








```{r}



library(xgboost)



```






```{r}



doParallel::registerDoParallel()

xgb_rs<-xgb_wf %>%
  fit_resamples(
    resamples=ug_folds,
    metrics=metric_set(roc_auc, accuracy, sensitivity, specificity,f_meas, precision),
    control=control_resamples(save_pred = TRUE)
  )





```






## Model Performance-Training Set

<br/><br/>

### Definitions

For this data set the positive class is "N" for enrolled_spring. This is
the event of interest for prediction. The negative category is "Y" for
did not enroll spring.

<br/><br/>







**Terms**

To evaluate model performance on predicting the True Positive class
("N") multiple plots and metrics will be used.

**TP** - True Positive. The model predicted positive and it's true.

**TN** - True Negative. The model predicted negative and it's true.

**FP** - False Positive. The model predicted positive and it's false.

**FN** - False Negative. The model predicted negative and it's false.

**ROC** - Probability curve that plots the TPR against FPR at various
threshold values.

**Accuracy** - Proportion of true results among the total number of
cases.

**Sensitivity** - Proportion of positive classes that are correctly
classified. Also known as *Total Positive Rate(TPR)* and *Recall.*

**Specificity** - Proportion of actual negatives that are correctly
classified.

**FPR** - Fall Positive Rate. Proportion of negative classes that are
incorrectly classified.

<br/><br/>








```{r}






```



