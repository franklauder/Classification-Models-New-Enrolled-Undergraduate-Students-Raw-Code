---
title: "Classification Models for New Enrolled Students"
author: "Frank Laudert"
date: "2022-12-19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Import R Libraries


```{r}




library(rmarkdown)

library(reticulate)

library(knitr)



library(dplyr)

library(ggplot2)

library(tidyr)

library(pander)

library(kableExtra)

library(forcats)

library(skimr)

library(gt)

library(GGally)

library(gmodels)

library(plotly)

library(scales)


library(ggforce)

library(ggdist)

library(lubridate)

library(DT)

library(visdat)



library(packcircles)

library(viridis)

library(ggthemes)

library(purrr)

library(ggtext)

library(RColorBrewer)

library(ggpubr)
```

test 123 

Import Python Libraries

```{python}




import pandas as pd

import numpy as np

import datetime


from dateutil.relativedelta import relativedelta

from datetime import date
```




# Introduction

<br/><br/>

<br/><br/>

This analysis will review different classification models on their
ability to predict new student persistence, i.e. do first time
undergraduate students entering for the fall term continue enrollment in
the spring term of the same academic year. Only students enrolled in
Bachelor degrees will be used for this analysis.

This report is designed so sections of interest can be accessed directly
from the index on the left. Code for this project is included for
interest sake but its understanding is not required for interpretation
of the visualizations and tables produced. The purpose of this project
is for any individual to understand the results of the models without
delving into how the models are built and fit.

R version 4.1.3 and Python version 3.8.12 are used for this for this
project.


```{r}

ug_v1<-readRDS("ug_v1")



```


# Initial Data Analysis

<br/><br/>

This section will check the structure and completeness of the data. In
other words we will check how many columns and rows the data set has and
if there are any missing values. Columns are the features (variables)
and rows are the observations.

<br/><br/>





```{r}


dim(ug_v1)


```






<br/><br/>

From the above output we find that the data set has 23 columns and 2,667
rows.

<br/><br/>

<br/><br/>








```{r include=FALSE}


class_str<-vis_dat(ug_v1)

```







```{r echo=FALSE, fig.cap="Figure 1: Variable Structure Plot", fig.height=8, fig.width=11, message=FALSE, warning=FALSE}


class_str




```




<br/><br/>

*Figure 1* visually presents the breakdown of variable types indicated
by color.

From this plot it is determined that there are 15 variables of type
*character,* 6 variables of type numeric and 2 variables of type date
(POSIXct). The color gray provides an overview of the depth of missing
(NULL) values for each variable.

<br/><br/>







```{r}



  
  
  mis_val<-vis_miss(ug_v1, cluster=TRUE,sort_miss = TRUE)


```







<br/><br/>

*Figure 2* provides an in depth analysis of missing (NULL) values for
the data set as a whole and for each individual variable. First, the
plot gives the total percentage of missing values in the entire data set
(3.7%). The plot presents more detail of missing values by providing
information on a per variable basis. The extent of missing values within
a specific variable is visually represented by black shaded areas.
Located at the top of the plot, beside each variable name, is the
percentage of missing values for that variable. *English Your Primary
Language* and Stage_dte features have by far the largest percentage
missing data at 31.9% and 26.43% respectively.

<br/><br/>

# Cleaning and Preparation

<br/><br/>

Cleaning and preparation involves removing duplicate rows/columns,
filling missing entries with statistical methods , dropping or combining
categories, renaming variables or categories, removing null entries, and
creating new variables.

<br/><br/>

## Categorical Features

<br/><br/>

From Figure 1 we know that there are 15 categorical variables in the
new_ug data set that are of type *`character`*. Categorical variables
must be of type *factor* for use in classification models, thus these
variables will be transformed to type *`factor`*.

<br/><br/>

<!--# mutate_if function from dplyr package will be useed to transform character variables to type factor -->

<br/><br/>



```{r}


ug_v2<-ug_v1 %>% 
   mutate_if(is.character, factor)



```








```{r}


class_str_2<-vis_dat(ug_v2)


```


Figure 2a shows all *categorical* variables colored in orange are now
type *factor*.

<br/><br/>







```{r}


v2_sum_1<-ug_v2 %>% 
  select_if(is.factor) %>% 
  summary()

```









```{r}

pander(v2_sum_1, "Table 1: new_ug_v2 Summary Factor Variables")



```







<br/><br/>

From *Table 1* it can be viewed that certain categorical variables have
categories that exhibit value inconsistency (representations of the same
category). These value inconsistencies are due to differences in
capitalization or different abbreviations. These categories will be
re-coded so as to eliminate value inconsistencies.

<br/><br/>

Categories for the *filed_fafsa* and *enrolled_spring* variables will be
re-coded so all are upper case.



<!--# The recode() function from dplyr package will be used to  change the values of factor levels -->





```{r}


ug_v3<- ug_v2 %>%
  mutate(filed_fafsa=recode(filed_fafsa, 'y' ="Y" )) %>% 
  mutate(enrolled_spring=recode(enrolled_spring, 'y' ="Y" )) %>% 
  mutate(english_your_primary_language=recode(english_your_primary_language, 'y' ="Y", "u"="unk", "Unk"="unk" )) 



```









```{r}


incon_var<-ug_v3 %>% 
  select(filed_fafsa, english_your_primary_language, 
         enrolled_spring) %>% 
  summary()


```








```{r}

pander(incon_var, "Table 2:  Summary Recoded Variables ")


```

<br/><br/>







Certain variables have unnecessary categories and thus need to be
reduced. This will be handled by collapsing smaller categories into
general and bigger categories.

The *value_description* variable has a number of categories with low
counts and as such these categories will be collapsed into others. All
low count categories will be collapsed into a a general category
"*other",* except for"*Hispanic/Latino"* , which will be collapsed into
"*Hispanic"*. Also, the name *value_description will be renamed to
race.*

<br/><br/>









```{r}


 ug_v3<-ug_v3 %>% 
  mutate(value_description=recode(value_description,"Race and Ethnicity unknown"="unkown" , "Two or more races"="other", 
     "American Indian or Alaska Native" ="other",
     "Native Hawaiian or Other Pacific Islander" = "other","Nonresident Alien"="other", "Hispanic/Latino"="Hispanics of any race"))


```








```{r}


ug_v3<-ug_v3 %>% 
  rename("race"="value_description")


```



```{r}


ug_v3<-ug_v3 %>% 
  mutate(race=recode(race,"Hispanics of any race"="Hispanic" , "Black or African American"="African American"))

```








```{r}

val_des_cnt_2<-ug_v3 %>%
  count(race,sort=TRUE, name="Total Count") 

```



```{r}


pander(val_des_cnt_2, caption= "Table 3: Race  Variable Count-Recoded")


```






```{r}


deg_cde_cnt<-ug_v3 %>%
  count(degr_cde,sort=TRUE, name="Total Count") 




```








```{r}

pander(deg_cde_cnt, caption= "Table 4: Degree Code Variable Count")


```




As seen from *Table 4* above, the *degree code* feature has a number of
categories that will be eliminated due to not being a bachelor degree.
As such, the *degree code* categories *CAGS, CT, AS*, and *PRECT* will
be omitted from the data set.


```{r}


ug_v3<-ug_v3 %>%  
  filter(degr_cde %in% c("BS", "BA", "BSMG", "BAMS", "BSHS", "BAPSY"))


```



Now that un-needed categories have been eliminated from ***degree
code***, the remaining categories will be collapsed into either *BS* or
*BA* to simplify analysis and visualization.

<br/><br/>


```{r}

ug_v3<- ug_v3 %>%
   mutate(degr_cde=recode(degr_cde, 'BSMG' ="BS", "BAMS"="BA", "BSHS"="BS","BAPSY"="BA"))

```





```{r}


degr_cde_cnt_2<-ug_v3 %>%
  count(degr_cde,sort=TRUE, name="Total Count") 


```



```{r}



ug_v3$degr_cde<-droplevels(ug_v3$degr_cde)
ug_v3$race<-droplevels(ug_v3$race)
```


```{r}



pander(degr_cde_cnt_2, caption= "Table 5: Degree Code Variable (Combined) count")


```


```{r}


prog_desc_cnt<-ug_v3 %>%
  count(prog_desc,sort=TRUE, name="Total Count") 
  
  

```


```{r}



 pander(prog_desc_cnt,caption= "Table 6: Program Description Count")
 
 
```



<br/><br/>

Categories in the *Program Description* feature will be shortened for
presentation purposes.

<br/><br/>

```{r}


ug_v3<- ug_v3 %>%
   mutate(prog_desc=recode(prog_desc, 'Multidisciplinary Studies' ="Multidisc Studies", "Early Childhood Education and Care"="Early Childhood Educ/Care",
           "Health Care Management"="Health Care Mgmt",
         "Bachelor of Science in Business Administration"="Business Admin",
         "Wellness and Health Promotion"="Wellness & Health Promotion",
         "Bachelor of Science in Healthcare Administration" = "Healthcare Admin", "Bachelor of Science in Accounting"="Accounting","Human Services Management"="Human Services Mgmt","Natural and Applied Sciences"="Natural & Applied Sciences",
         "Bachelor of Science in Quality Syst & Imprv Mgmt"="Quality Syst & Imprv Mgmt","Bachelor of Science in Quality Systems Management"="Quality Systems Mgmt","BA in Multidisciplinary Studies (CCG)"="Multidisc Studies", 
         "Bachelor of Science in Digital Marketing"="Digital Marketing",
         "Information Technology Management"="Information Technology Mgmt",
         "Management Studies"="Mgmt Studies"))


```



```{r}


prog_desc_cnt_2<-ug_v3 %>%
  count(prog_desc,sort=TRUE, name="Total Count")
```



```{r}


pander(prog_desc_cnt_2, "Table 7: Program Description Count (Category Names Shortened")
```


## Numeric Variables

```{r}


num_sum<-ug_v3 %>% 
  select_if(is.numeric) %>% 
  summary()
```


```{r}

pander(num_sum,caption= "Table 8 Numeric Variables Summary")


```



<br/><br/>

*Table 8* shows the feature *hrs_enrolled* has a minimum of zero which
indicates a student did not enroll for the fall term. Since the analysis
is based on students who have enrolled at the college the *hrs_enrolled*
feature will be filtered to determine how many observations have zero
for *hrs_enrolled*.

<br/><br/>



```{r}

ug_v3 %>% select(hrs_enrolled) %>% summary()


```




<br/><br/>

Any observation that has *hrs_enrolled* as zero will be removed.

<br/><br/>




```{r}



ug_v4<-ug_v3 %>% 
  filter(hrs_enrolled > 0)



```








```{r}



ug_v4 %>% 
  select(hrs_enrolled) %>% 
  filter(hrs_enrolled==0)


```



## Feature Creation

<br/><br/>

New features will be created to enhance the analysis and modeling.

<br/><br/>






<br/><br/>

*Python* will be used for the creation of certain features. In order for
*Python* to be used on a R data frame within *the R Markdown*
environment, the *R* data.frame must be converted to a *Pandas* data
frame.

<br/><br/>


<!--# New features will be created to enhance the analysis and modeling.   -->

<!--#   Python will be used for the creation of certain features. In order for Python to be used on a R data frame within the R Markdown environment, the R data.frame must be converted to a Pandas data frame.   -->




```{python}


new_enroll_p=r.ug_v4


```




```{python}



new_enroll_p.info()
```







<br/><br/>

<!--# The feature stageDT_classStart_diff will be created from the difference between the date a student was accepted (hist_stage_dte) and the class start date. The time difference is reported in days.  -->

<br/><br/>





```{python}



new_enroll_p["accpt_Start_diff"]=(new_enroll_p["stage_dte"]-new_enroll_p["class_st_dte"]).dt.days




```







```{python}


new_enroll_p["accpt_Start_diff"].head()

```






```{python}



new_enroll_p["accpt_Start_diff"].describe()


```





<br/><br/>

<!--#  The feature age will be created by calculating the difference between the date of the term a student started and that student's date of birth. This new feature is the age of the student at the start of their first fall term of enrollment. -->






```{python}



new_enroll_p["age"]=(new_enroll_p["class_st_dte"]-new_enroll_p["birth_dte"])

new_enroll_p["age"]=new_enroll_p["age"]/np.timedelta64(1,'Y')



```







```{python}


new_enroll_p[["age"]].info()




```







```{python}



new_enroll_p["age"].head()

```







<br/><br/>

<!--#  The above output shows age has six decimal points. The will be rounded up to two decimal points.  -->

<br/><br/>






```{python}








new_enroll_p["age"]=new_enroll_p["age"].round()



```








```{python}


new_enroll_p["age"].head()



```





<br/><br/>

<!--#  The feature percent of enrolled credits earned by dividing credits earned by credits attempted. -->








```{python}



new_enroll_p["pct_enrolled_cr_earned"]=(new_enroll_p["trm_hrs_earned"]/new_enroll_p["hrs_enrolled"])


```






```{python}


new_enroll_p["pct_enrolled_cr_earned"].head()


```







```{python}



new_enroll_p["pct_enrolled_cr_earned"].describe()




```








<br/><br/>

<!--#   The Pandas data frame will be converted to a R data.frame   -->

<br/><br/>


```{python}



r.ug_v5=new_enroll_p



```






<!--# We will switch back to R chunks -->




<br/><br/>

A new categorical variable will be created from the numeric variable
*hrs_enrolled*.

<br/><br/>



```{r}


ug_v8<-ug_v8 %>% 
  mutate(term_enrolled_sts=case_when(
    between(hrs_enrolled, 1,5) ~ "less than half time",
    between(hrs_enrolled, 6,8) ~ "half time",
    between(hrs_enrolled, 9,11)~ "three quarter time",
    TRUE ~ "full time"))


```





```{r}


ug_v8$term_enrolled_sts<-as.factor(ug_v8$term_enrolled_sts)



```







```{r}



levels(ug_v8$term_enrolled_sts)



```



<br/><br/>

<!--#   The new categorical variable term_hrs_sts should be ordinal yet the output of the levels show"three quarter time" is located in the fourth slot. Its correct slot should be 2nd behind the "full time" category. The categories will be re-leveled to remedy this.   -->

<br/><br/>






<!--# The fct_relevel() function from the forcats package will be used change the levels  -->


```{r}


ug_v8<-ug_v8%>% 
  mutate(term_enrolled_sts=fct_relevel(term_enrolled_sts,"three quarter time", after=1))



```



```{r}



ug_v8$prog_cde<-droplevels(ug_v8$prog_cde)



```







<br/><br/>

<!--#  Year of class start variable will be created by extracting year from the class_st_dte variable.    -->


<br/><br/>






```{r}


ug_v8<-ug_v8 %>% 
  mutate(year=lubridate::year(class_st_dte))



```







```{r}



ug_v9<-ug_v8 %>% 
  filter(race %in% c("Hispanics of any race", "Black or African American", "White", "Asian", "other"))



```








<br/><br/>

# Exploratory Data Analysis

<br/><br/>

<br/><br/>











<!--# Create count data frame for loc_perc plot  -->



```{r}




gender_years<-ug_v9 %>%
  count(year, gender) %>%
  #mutate(perc = round(n / sum(n),2)) %>% 
  complete(gender,year, fill=list(n=0))
  

cumulative_gender<-gender_years %>% 
  split(f=.$year) %>% 
  accumulate(.,~bind_rows(.x, .y)) %>% 
  bind_rows(.id="frame")




```





```{r}



gend_years_plt<-cumulative_gender %>% 
  plot_ly(x=~year, y=~n, color=~gender) %>% 
  add_lines(frame=~frame, ids=~gender)



```





```{r echo=FALSE,fig.cap="Figure 3: Enrollment by gender(2010-2021)"", message=FALSE, warning=FALSE,fig.height=6, fig.width=6 }



gend_years_plt

```






```{r}


gen_per<-ug_v9 %>% 
  filter(year >2015) %>% 
  group_by(year) %>% 
  count(gender) %>% 
  mutate(perc = round(n / sum(n),2)) 



```






```{r}





gen_plt<-ggplot(gen_per, aes(x = reorder(gender, -perc), y = perc, fill=gender))+ 
  geom_bar(stat = "identity")+
  ggtitle("Enrollment by Gender (2016-2021) ")+
  labs( fill = "Gender")+
  scale_y_continuous(labels=scales::percent_format())+
  geom_text(aes(label=percent(perc),
                y=perc + .01),
            position=position_dodge(0.2),
            vjust=.05)+
  theme(axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        plot.title=element_text(size=10, hjust=0.5))+
  facet_wrap(~year)


```





```{r echo=FALSE, fig.cap="Figure 4: Percentage of Enrollment by gender(2016-2021)", fig.height=4, fig.width=4, message=FALSE, warning=FALSE}



gen_plt



```






<br/><br/>

*Figure 3* shows enrollment split by gender over time from 2010 to 2021.
The trend for male student appears to decline relative to total
enrollment after 2015. **Figure 4** concentrates on the years on the
years 2016 to 2021. From the bar plots we can see that male enrollment
as a percentage of total enrollment decreased by 6 to 7 percent starting
at 2018 and every year after.

<br/><br/>








```{r}


race_years<-ug_v9 %>% 
  count(year, race) %>% 
  complete(race, year, fill=list(n=0))

cumulative_race<-race_years %>% 
  split(f=.$year) %>% 
  accumulate(.,~bind_rows(.x, .y)) %>% 
  bind_rows(.id="frame")



```







```{r}





ethn_time_plt<-cumulative_race %>% 
  plot_ly(x=~year, y=~n, color=~race) %>% 
  add_lines(frame=~frame, ids=~race)



```





```{r echo=FALSE,fig.cap="Figure 5 fig.height=5, fig.width=6, message=FALSE, warning=FALSE}




ethn_time_plt





```









```{r}



eth_per<-ug_v9 %>% 
  filter(year >2015) %>% 
  group_by(year) %>% 
  count(race) %>% 
  mutate(perc = round(n / sum(n),2)) 



```







```{r}




eth_plt<-ggplot(eth_per, aes(x = reorder(race, -perc), y = perc, fill=race))+ 
  geom_bar(stat = "identity")+
  ggtitle("Enrollment by Race (2016-2021) ")+
  labs(x = "Race", y = "Percent", fill = "Race")+
  scale_y_continuous(labels=scales::percent_format())+
  theme(
    axis.ticks.x=element_blank(),
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.x=element_blank(),
        axis.text.y=element_text(size=7),
        plot.title=element_text(size=10, hjust=0.5),
    legend.title=element_text(size=10))+
  facet_wrap(~year)



```






```{r echo=FALSE, fig.cap="Figure 6", fig.height=4, fig.width=4, message=FALSE, warning=FALSE}



eth_plt


```






<br/><br/>

*Figure 5* shows enrollment split by race over time from 2010 to 2021.
From this plot we can see enrollment for African American and Hispanic
students dropped more dramatically than other groups. *Figure 6* shows
enrollment of Hispanic students as a percentage of total enrollment
declining at 2019 and dropping below thirty percent in 2020 and 2021.
African American enrollment as a percentage of total enrollment declined
below thirty strarting at 2017 and for every year to 2021.

<br/><br/>








```{r}




fafsa_rac_plt<-ug_v9 %>% 
  group_by(race,year) %>% 
  count(filed_fafsa) %>% 
  mutate(perc = round(n / sum(n),2)) %>% 
  ggplot(aes(x=reorder(x=race, -perc),y=perc, fill=filed_fafsa))+ 
  geom_bar(stat="identity")+
   ggtitle("Financial Aid Filing Status by Race and Year")+
  labs(x="Race", fill="Filed FAFSA")+
  scale_y_continuous(labels=scales::percent_format())+
  theme(axis.ticks.x=element_blank())+
  theme(axis.text.x=element_text(angle=70, hjust=1),
        axis.text.y=element_text(size=7),
        axis.title.y=element_blank(),
        axis.title.x=element_blank(),
        legend.title=element_text(size=9),
        legend.text=element_text(size=6),
        plot.title=element_text(size=10, hjust=0.5))+
  facet_wrap(vars(year))



```














```{r echo=FALSE, fig.cap="Figure 7", fig.height=5, fig.width=5}




fafsa_rac_plt



```







<br/><br/>



```{r}



prog_desc_sub<-ug_v9 %>% 
  count(prog_desc) %>% 
  mutate(perc = round(n / sum(n),2)) 





```








```{r}




prog_desc_perc_2<-ggplot(prog_desc_sub, aes(x =reorder(prog_desc), y = perc, group=1, text=paste("Program Description: ",prog_desc,"<br>Percent: ", perc)))+ 
  geom_point(
    stat="identity",size=3,color="orange")+
  ggtitle("Program Enrollment")+
  labs(x = "Program Description", y = "Percent")+
  scale_y_continuous(labels=scales::percent_format())+
  theme(axis.text.x=element_text(angle=90, hjust=1),
        axis.title.x=element_blank(),
    axis.title.y=element_blank(),
    plot.title=element_text(size=10, hjust=0.5))+
  coord_flip()




```










```{r echo=FALSE, fig.cap="Figure 8", fig.height=8, fig.width=8, message=FALSE, warning=FALSE}




ggplotly(prog_desc_perc_2,tooltip = "text")

```










```{r}




prog_desc_plot <- ggplot(ug_v9, aes(y=prog_desc, x=gender, color=gender,group=prog_desc)) +
  geom_count(alpha=0.5) +
  labs(title = "Programs Split by Gender",
       size = "14")+
   theme(
    axis.title.x=element_blank(),
    axis.title.y=element_blank(),
    plot.title=element_text(size=9, hjust=0.5),
    axis.text.x=element_blank()
  )



```










```{r echo=FALSE, fig.cap="Figure 9"}



ggplotly(prog_desc_plot)


```











```{r}



prog_top<-ug_v9 %>% 
  filter(prog_desc %in% c("Mgmt Studies", "Human Services", "Multidisc Studies",
                          "Psychology", "Early Childhood Educ/Care", 
                          "Health Care Mgmt")) %>% 
  group_by(year) %>% 
  count(prog_desc)



```











```{r}


prog_line_plt<-prog_top %>% 
  ggplot(aes(x=year, y=n, group=prog_desc, color=prog_desc, text=paste("Program Description: ",prog_desc,"<br>Count: ", n)))+
  ggtitle("Top Six Program Enrollment 2010-2021")+
  labs( color = "Program Description")+
  theme(
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.x=element_text(size=6),
         axis.text.y=element_text(size=8),
        legend.text=element_text(size=6),
        legend.title=element_text(size=7),
        plot.title=element_text(size=10, hjust=0.5))+
  geom_line(alpha=0.4)




```












```{r echo=FALSE, fig.cap="Figure 9", fig.height=8, fig.width=6, message=FALSE, warning=FALSE}



ggplotly(prog_line_plt,tooltip = "text")




```









<br/><br/>

*Figure 8* displays program enrollment as a percentage of total
enrollment over twelve years. From this plot it is clear that Management
Studies is the largest program with almost one fourth of the total
enrollment. Figure 9 breaks the program down by gender. We can see that
Management Studies has large enrollments for both men and women. After
Management studies it appears women enrollment is diversified among
other programs comparpred to men. Figure 10 shows enrollment trend over
time of the six largest enrolled programs. Though all programs declined
in enrollment, Management Studies and Multidisciplinary studies had
dramatic declines in enrollment.

<br/><br/>











<br/><br/>











```{r}




gender_enroll_sts<-ug_v9 %>% 
   count(term_enrolled_sts, gender) %>%
  #mutate(prop=n/sum(n)) %>%
 # group_by(term_enrolled_sts) %>%
  plot_ly(x=~term_enrolled_sts, y=~n,color=~gender,hoverinfo="y") %>% 
  add_bars() %>% 
  layout(title = 'Term Enrollment Status Split by Gender', xaxis = list(title = 'Term Enrollment Status'), font=t, plot_bgcolor = "#e5ecf6",

         yaxis = list(title = 'Count'), legend = list(title=list(text='Gender', barmode='group')))




```










```{r echo=FALSE, fig.cap="Figure 10",fig.height=7, fig.width=6, message=FALSE, warning=FALSE}


gender_enroll_sts




```









```{r}



enr_sts_age_gender<-ug_v9 %>% 
  ggplot(aes(x=term_enrolled_sts, y=age, fill=term_enrolled_sts))+
  geom_boxplot()+
  facet_wrap(~gender)+
   labs(x = "Enrollment Status", y = "Age")+
  theme(legend.position = "none")






```






```{r echo=FALSE, fig.cap="Figure 11", fig.height=7, fig.width=6, message=FALSE, warning=FALSE}



enr_sts_age_gender



```















<br/><br/>



<br/><br/>








```{r}


p_2<-ggplot(ug_v9, aes(x=age, y=gender)) + 
  geom_jitter(position=position_jitter(0.2))+
  ggtitle("Strip Chart Age by Gender")




```







```{r}


pt_2<-p_2 + coord_flip()

pa_2<-pt_2 + stat_summary(fun=median, geom="point", shape=18,
                 size=3, color="red")

pa_pt_2<-pa_2+scale_color_grey() + theme_classic()



```








```{r echo=FALSE,  fig.cap="Figure 13: Strip Chart-Age by Gender", message=FALSE, warning=FALSE}



pa_pt_2


```












<br/><br/>

*Figure 11* shows that over half of all female students were enrolled
half-time (6-8 credits) while males were more evenly distributed with
half-time enrollment at 40% and full-time at 33%. *Figure 12* displays
goes further by adding the age feature to the equation via a box plot.
For the purpose of this analysis there are only two parts of the box
plot we need two focus on. The line within the box is the median. Fifty
percent of all age values are above the line and fifty percent of all
age values are below the line. The length of the box includes fifty
percent of population for that specific box plot. For both females and
males the median age increases as enrolled credits decrease. The only
difference between men and women is the half time box plot. Fifty
percent of women for Half time enrollment status fall between ages 26
and 49 whereas fifty percent of men fall between ages 27 and 40. This
may be important later on when the classification models are evaluated.
*Figure 13* displays age distribution of gender in a simplified form.
From this plot we can observe that both men and women are heavily
concentrated under the age of thirty. Women distribution of age thin out
between forty and fifty compared to men whose age distribution thins out
right after thirty.

<br/><br/>








```{r}



ug_stat<-ug_v9

ug_stat$term_enrolled_sts<-factor(ug_stat$term_enrolled_sts,                                    # Change ordering manually
                  levels = c("full time", "three quarter time", 
                             "half time", "less than half time"))



```









```{r}


enroll_year_plt<-ug_v9 %>% 
  group_by(term_enrolled_sts,year) %>% 
  count(gender) %>% 
  mutate(perc = round(n / sum(n),2)) %>% 
  ggplot(aes(x=reorder(x=term_enrolled_sts, -perc),y=perc, fill=gender))+ 
  geom_bar(stat="identity")+
   ggtitle("Enrollment Status by Gender and Year")+
  labs(x="Gender", fill="Enrollment Stats")+
  scale_y_continuous(labels=scales::percent_format())+
  theme(axis.ticks.x=element_blank())+
  theme(axis.text.x=element_text(angle=70, hjust=1),
        axis.text.y=element_text(size=7),
        axis.title.y=element_blank(),
        axis.title.x=element_blank(),
        legend.title=element_text(size=8),
        legend.text=element_text(size=6),
        plot.title=element_text(size=10, hjust=0.5))+
  facet_wrap(vars(year))


```










```{r echo=FALSE, fig.cap="Figure 14"}




enroll_year_plt



```






<br/><br/>

Figure 14 views enrollment status by each year. Of interest is the full
time enrollment of men. For the years 2012 through 2017, full time
enrollment hovered between thirty and fifty percent. After 2017, full
time enrollment for men dropped below thirty percent.

<br/><br/>






```{r}



trns_cred_box<-ug_v9 %>% 
   filter(race %in% c("Hispanic","African American","White","Asian")) %>% 
  ggplot(aes(x=race, y=xfer_hrs_earned, fill=race))+
  geom_boxplot()+
  ggtitle("Transfer Credits by Race")+
  facet_wrap(vars(race))+
  theme(legend.position = "none",
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.x=element_blank(),
        plot.title=element_text(size=10, hjust=0.5))



```













```{r echo=FALSE, fig.cap="Figure 15"}


ggplotly(trns_cred_box)




```















```{r}




tran_cred<-ug_v9 %>% 
  select(race, xfer_hrs_earned, year) %>% 
  filter(race %in% c("Hispanic","African American","White","Asian")) %>% 
  group_by(race, year) %>% 
mutate(median_tr=median(xfer_hrs_earned),
       q1=quantile(xfer_hrs_earned, p=.25),
       q3=quantile(xfer_hrs_earned, p=.75))





```






```{r}




tr_cr_plt<-ggplot(tran_cred,aes(x=year, y=median_tr,group=1,text=paste("Year: ",year, "Median Transfer Credits: ",median_tr)))+
  geom_line(color="red")+
  geom_line(aes(x=year, y=q1))+
  geom_line(aes(x=year, y=q3))+
  geom_hline(yintercept=3)+
  #geom_ribbon(aes(ymin=q1, ymax=q3))+
  ggtitle("Transfer Credits by Year and Race")+
  facet_wrap(vars(race))+
    theme(
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.x=element_text(size=6),
         axis.text.y=element_text(size=7),
        plot.title=element_text(size=10, hjust=0.5))+
  facet_wrap(vars(race))


```







```{r echo=FALSE, echo=FALSE, fig.cap="Figure 16"}



ggplotly(tr_cr_plt)



```





br/><br/>

*Figure 15* displays transfer credits by race. In terms transfer credit
hours, the Asian student population has a median transfer credit amount
of 54 which is 20 more than the other student groups. Transfer credits
are of importance as more credits will shorten the time to degree
completion and may increase the amount of financial aid a student
receives. Figure 16 presents transfer credits by race over time. This
may help point out changes in by year.

<br/><br/>






```{r}




gpa_box<-ug_v9 %>% 
   filter(race %in% c("Hispanic","African American","White","Asian")) %>% 
  ggplot(aes(x=race, y=trm_gpa, fill=race))+
  geom_boxplot()+
  ggtitle("GPA by Race")+
  facet_wrap(vars(race))+
  theme(legend.position = "none",
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.x=element_blank(),
        plot.title=element_text(size=10, hjust=0.5))



```
















```{r echo=FALSE, fig.cap="Figure 17,fig.height=11, fig.width=11,  message=FALSE, warning=FALSE}


ggplotly(gpa_box)


```















```{r}





gpa_yr<-ug_v9 %>% 
  select(race, trm_gpa, year) %>% 
  filter(race %in% c("Hispanic","African American","White","Asian")) %>% 
  group_by(race, year) %>% 
mutate(median_gpa=median(trm_gpa))



```






```{r}





gpa_yr_plt<-ggplot(gpa_yr,aes(x=year, y=median_gpa,group=1,text=paste("Year: ",year, "Median GPA: ",median_gpa)))+
  geom_line(color="red")+
  #geom_ribbon(aes(ymin=q1, ymax=q3))+
  ggtitle("GPA by Year and Race")+
  facet_wrap(vars(race))+
    theme(
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.x=element_text(size=6),
         axis.text.y=element_text(size=7),
        plot.title=element_text(size=10, hjust=0.5))+
  facet_wrap(vars(race))


```














```{r echo=FALSE, fig.cap="Figure 18"}





ggplotly(gpa_yr_plt)


```








```{r}





eth_enroll<-ug_v9 %>% 
  filter(race %in% c("Hispanic","African American","White","Asian")) %>% 
  count(race, enrolled_spring) %>% 
  group_by(race) %>% 
  mutate(perc = n / sum(n)) %>% 
  ggplot(aes(x=race, y=perc, fill=enrolled_spring))+
  geom_col(position="dodge")+
  ggtitle("Spring Enrollment by Race")+
  labs(x = "Race", y = "Percent", fill = " Enrolled Spring")+
  scale_y_continuous(labels=scales::percent_format())+
  #geom_text(aes(label=percent(perc),
                #y=perc + .03),
           # position=position_dodge(.9),
            #vjust=.05)+
  theme(
    axis.ticks.x=element_blank(),
    axis.text.x=element_text(angle=70, hjust=1),
    axis.title.y=element_blank(),
    axis.title.x=element_blank(),
    plot.title=element_text(size=10, hjust=0.5),
    legend.title=element_text(size=9),
    legend.text=element_text(size=7))





```






```{r echo=FALSE, fig.cap="Figure 19"}



eth_enroll




```










```{r}




spr_race<-ug_v9 %>% 
  filter(race %in% c("Hispanic","African American","White","Asian")) %>% 
  group_by(race, year) %>% 
  count(enrolled_spring) %>% 
  mutate(perc = round(n / sum(n),2)) %>% 
  ggplot(aes(x=year, y=perc, color=enrolled_spring,
             group=1,text=paste("Year: ",year, "Percent: ",perc, "Enrolled Spring:", enrolled_spring)))+ 
  geom_line(stat="identity")+
  ggtitle("Spring Enrolled by Year and Race")+
  labs(color = " Enrolled Spring")+
   scale_y_continuous(labels=scales::percent_format())+
  theme(
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.x=element_text(size=6),
         axis.text.y=element_text(size=7),
        legend.text=element_text(size=6),
        legend.title=element_text(size=7),
        plot.title=element_text(size=10, hjust=0.5))+
  facet_wrap(vars(race))



```










```{r echo=FALSE, fig.cap="Figure 20"}


ggplotly(spr_race,tooltip = "text")


```











```{r}





corr_plot_2<-ug_v9 %>% 
  select(enrolled_spring,hrs_enrolled, trm_gpa, xfer_hrs_earned, 
    accpt_Start_diff,age, pct_enrolled_cr_earned) %>% 
  ggscatmat(color="enrolled_spring", 
            corMethod = "pearson",
            alpha=0.2)




```








```{r echo=FALSE, fig.cap="Figure 21"}


corr_plot_2



```





<br/><br/>

From *Figure 3* we find that there is only one relationship that has a
very high correlation, pct_enrolled_cr_earned and hrs_enrolled. The
correlation plot splits the relationship by the different outcomes of
the response variable: *enrolled_spring.* Outcome of *yes* has a high
correlation of .87 while an outcome of *No* has a very high correlation
of .92. All other correlations either either low or negligible.

<br/><br/>

<br/><br/>

<br/><br/>

The data set will be filtered to include only features that will be
included in the classification models.




```{r}




ug_v10<-ug_v9 %>% 
  select(enrolled_spring,term_enrolled_sts, race, accpt_Start_diff, age, xfer_hrs_earned, filed_fafsa, trm_gpa, pct_enrolled_cr_earned, gender, degr_cde) %>%  mutate_if(is.character, factor)



```








```{r}


 ug_v10<-ug_v10 %>% 
  filter(race %in% c("Asian", "African American",
                                  "Hispanic","White"))





```




```{r}



names(ug_v10)



```





```{r}


ug_v10$race<-droplevels(ug_v10$race)



```






```{r}


saveRDS(ug_v10, "ug_v10.rds")


```




```{r}


ug_v10<-readRDS("ug_v10")


```








<br/><br/>

# Models

<br/><br/>

## Pre-processing / feature engineering

<br/><br/>







```{r}


library(tidymodels)



library(juicr)


```




<br/><br/>

The data will be split into training and testing sets. <br/><br/>


```{r}


set.seed(20)
new_ug_split<-initial_split(ug_v10, prop=0.70,strata=enrolled_spring)


```







```{r}


new_ug_training<-training(new_ug_split)


```





```{r}


new_ug_test<-testing(new_ug_split)



```




<br/><br/>

75% of the data has been split into the training set and 25% into the
testing set.

<br/><br/>







```{r}


dim(new_ug_training)


dim(new_ug_test) 





```





<br/><br/>

The next step is to partition the training set into equal subsets. The
subsets will be used to assess a models performance on training data
through cross validation.

The process works by setting aside the first fold as a test set and the
remaining subsets are used as the aggregated training set. The model is
trained on the aggregated training set then the performance is evaluated
on the testing set. This will continue until all folds have been held
out as a test set. An evaluation metric is calculated for each iteration
then averaged together. This results in a cross validated metric.

<br/><br/>







```{r}


set.seed(100)

ug_folds<-vfold_cv(new_ug_training, v=5)


```











```{r}


ug_folds



```



```{r}


library(themis)


```




<!--#  The recipe function is used for performing feature engineering.    -->

<!--#  step_zv removes any feature that has a zero variance.   -->

<!--# step_log() will log transform data (since some of our numerical variables are right-skewed)    -->

<!--#  step_normalize converts numerical features to the same scale.  -->

<!--#  step_dummy transforms the categorical features into dummy variables.  -->

<!--#  step_smote up samples the target feature using nearest neighbor algorithm. -->












```{r}


new_rec<-recipe(enrolled_spring ~ ., data=new_ug_training) %>% 
  #step_zv(all_numeric()) %>% 
  step_log(age) %>% 
  step_normalize(all_numeric()) %>% 
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_smote(enrolled_spring) 


```






<br/><br/>

<!--# Check if all of the pre-processing steps from above actually worked,    -->

<br/><br/>








```{r}


prepped_data <- 
  new_rec %>% # use the recipe object
  prep() %>% # perform the recipe on training data
  juice() # extract only the preprocessed dataframe 



```








```{r}


new_rec %>%  prep() %>% bake(new_data=NULL) %>% count(enrolled_spring)


```






<!--#  The above output verifies the target feature enrolled_spring classes have the same number of observations.  -->





```{r}



glimpse(prepped_data)

```





<!--#  The glimpse function shows all numeric features have been scaled and all categorical features are now in binary form.  -->







```{r}



Prepped_corrplt<-prepped_data %>% 
  select(xfer_hrs_earned,
         age,
         accpt_Start_diff,
         ) %>% 
  ggscatmat(corMethod = "spearman",
            alpha=0.2)


```








```{r}



Prepped_corrplt


```







## Model Descriptions

<!--#   First the model will be built. This includes setting the specification. Certain model specifications will be determined by a tuning grid.  -->

<!--#   Once the specification is set the models will be fit to the validation set (cv_folds) and used to estimate the performance of each model using the fit_resamples() function.  -->

<!--#   fit_resamples() will fit a model to each re-sample and evaluate on the held out sample (test set). This repeats until all re-sample has been fit as the test set.  -->







<br/><br/>

### Decision Tree

Decision trees use a flowchart like a tree structure to show the
predictions that result from a series of feature splits. In order to
accomplish this, a decision tree is made up of three types of nodes:

-   Root Node (parent node): The node that starts the graph. It
    evaluates the variable that best splits the data.

-   Intermediate Nodes (child nodes): These are nodes where features are
    evaluated for further splits of the data but are not the final
    nodes.

-   Leaf nodes (terminal nodes): These are the final nodes of the tree,
    where the prediction of a categorical event are made.

For a more detailed explanation of decision trees check the link below.

[Guide to Decision
Trees](https://www.analyticsvidhya.com/blog/2021/08/decision-tree-algorithm/)








```{r}


library(rpart)



```





<!--#  Set the specifications for the decision tree model  -->







```{r}


doParallel::registerDoParallel()

tree_spec_untuned <- decision_tree(
  min_n=tune(),
tree_depth=tune(),
cost_complexity = tune()) %>% 
      set_engine('rpart') %>% 
      set_mode('classification')



```








```{r}



tree_grid<-grid_regular(extract_parameter_set_dials(tree_spec_untuned),levels=3)


```






```{r}



tree_tune_results<-tune_grid(
  tree_spec_untuned, 
  enrolled_spring~., 
  resamples=ug_folds
  #metrics=metric_set(accuracy, roc_auc)
)



```





```{r}


final_tree_params<-select_best(tree_tune_results, "roc_auc")



```






```{r}



final_tree_params



```








```{r}



tree_params_table<-ggtexttable(final_tree_params, rows = NULL, 
                        theme = ttheme("mBlue", base_size=11)) %>% 
   tab_add_title("Final Decision Tree Parameters",face="bold") 




```







```{r}



 tree_params_table


```







```{r}

tree_spec<-finalize_model(tree_spec_untuned, final_tree_params)



```





<br/><br/>

<!--#   The two previous steps will be combines using workflows. A workflow combines the pre-processing recipe and model specification.   -->

<!--#    By creating a workflow, all of the pre-processing will be handled during the fitting of the model and when generating new predictions. Workflows are equivalent to Python's Scikit Learn's pipeline.  -->






```{r}




tree_wf<-workflow() %>% 
  add_model(tree_spec) %>% 
  add_recipe(new_rec)




```




<!--#  Fit decision tree model to cross validation folds  -->






```{r}



doParallel::registerDoParallel()

tree_rs<-tree_wf %>% 
  fit_resamples(
    resamples=ug_folds,
    metrics=metric_set(roc_auc, accuracy, sensitivity, specificity, f_meas, precision),
    control=control_resamples(save_pred = TRUE)
  )




```






### Random Forest

Random Forest models are made up of individual decision trees whose
predictions are combined for a final result. The final result is decided
using majority rules which means that the final prediction is what the
majority of the decision tree models chose. Random Forests can be made
up of thousands of decision trees.

Simply put, the random forest builds multiple decision trees and merges
them together to get a more accurate prediction.

[Random Forest for
Beginners](https://www.analyticsvidhya.com/blog/2021/10/an-introduction-to-random-forest-algorithm-for-beginners/)

<!--#   Set the specifications for the random forest model  -->







```{r}

library(randomForest)



```






```{r}


rf_spec_untuned <- rand_forest(
 # mtry=tune(),
  trees=tune(),
  min_n=tune()) %>% 
      set_engine('randomForest') %>% 
      set_mode('classification')



```






```{r}



rf_grid<-grid_regular(extract_parameter_set_dials(rf_spec_untuned),levels=3)



```







```{r}




doParallel::registerDoParallel()

rf_tune_results<-tune_grid(
  rf_spec_untuned, 
  enrolled_spring~., 
  resamples=ug_folds
  #metrics=metric_set(accuracy, roc_auc)
)


```





```{r}



final_rf_params<-select_best(rf_tune_results, "roc_auc")



```







```{r}


final_rf_params



```








```{r}




rf_params_table<-ggtexttable(final_rf_params, rows = NULL, 
                        theme = ttheme("mGreen", base_size=11)) %>% 
   tab_add_title("Final Random Forest Parameters",face="bold")


```







```{r}


rf_params_table



```







```{r}




rf_spec<-finalize_model(rf_spec_untuned, final_rf_params)



```








```{r}






rf_wf<-workflow() %>% 
  add_model(rf_spec) %>% 
  add_recipe(new_rec)



```





<!--#  Fit the models to the re-sampled folds.  -->






```{r}





doParallel::registerDoParallel()

rf_rs<-rf_wf %>% 
  fit_resamples(
    resamples=ug_folds,
    metrics=metric_set(roc_auc, accuracy, sensitivity, specificity, f_meas, precision),
    control=control_resamples(save_pred = TRUE)
  )



```







### Logistic Regression

<br/><br/>

Logistic Regression is a binary classification model that finds the
probability or odds ratio of an event. For this model there are two
events or possible outcomes, yes or no. A probability between 0 and 1 of
an observation belonging to one of the two categories is produced.

If you would like to know more about Logistic Regression check out the
link below.

[Logistic Regression for
Beginners](https://www.analyticsvidhya.com/blog/2021/08/conceptual-understanding-of-logistic-regression-for-data-science-beginners/)

<!--#  Set the specifications for the logistic regression model  -->






```{r}




log_reg_spec<- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")



```








```{r}





logreg_wf<-workflow() %>% 
  add_model(log_reg_spec) %>% 
  add_recipe(new_rec)




```






<!--#  Fit Logistic Regression model to re-sampled  folds  -->


```{r}





doParallel::registerDoParallel()

log_rs<-logreg_wf %>% 
  fit_resamples(
    resamples=ug_folds,
    metrics=metric_set(roc_auc, accuracy, sensitivity, specificity, f_meas, precision),
    control=control_resamples(save_pred = TRUE)
  )




```




### KNN

<br/><br/>

Nearest neighbor methods work by taking the value of an observation's
attribute (known also as features or variables) and then locating
another observation whose attribute value is numerically nearest to it.

More on the Nearest Neighbor algorithm can be found at the following
link.

[Introduction to KNN
Algorithms](https://www.analyticsvidhya.com/blog/2022/01/introduction-to-knn-algorithms/)






```{r}



knn_spec<-nearest_neighbor() %>% 
  set_engine("kknn") %>% 
  set_mode("classification")



```





```{r}



knn_wf<-workflow() %>% 
  add_model(knn_spec) %>% 
  add_recipe(new_rec)


```




```{r}

doParallel::registerDoParallel()

knn_rs<-knn_wf %>% 
  fit_resamples(
    resamples=ug_folds,
    metrics=metric_set(roc_auc, accuracy, sensitivity, specificity,f_meas, precision),
    control=control_resamples(save_pred = TRUE)
  )



```







### XGBoost

Gradient boosting ,just as random forest, is made up of many individual
decision trees. The difference is instead of using the majority vote of
the combined decision trees it adjusts a decision tree model based on
the incorrect predictions from the decision tree before. New trees are
fit based entirely on the errors from the previous tree's predictions.
That is to say predictions that are accurate are not included in the fit
of new trees. New trees are added sequentially until no further
improvements can be made.

More information of the Xgboost algorithm can be found at the below
link.

[Guide to
XGBoost](https://www.analyticsvidhya.com/blog/2018/09/an-end-to-end-guide-to-understand-the-math-behind-xgboost/)














```{r}





xgb_spec<-boost_tree(trees=2000, mtry=5) %>% 
  set_mode("classification") %>% 
  set_engine("xgboost")




```







```{r}



xgb_wf<-workflow() %>% 
  add_model(xgb_spec) %>% 
  add_recipe(new_rec)




```








```{r}



library(xgboost)



```






```{r}



doParallel::registerDoParallel()

xgb_rs<-xgb_wf %>%
  fit_resamples(
    resamples=ug_folds,
    metrics=metric_set(roc_auc, accuracy, sensitivity, specificity,f_meas, precision),
    control=control_resamples(save_pred = TRUE)
  )





```






## Model Performance-Training Set

<br/><br/>

### Definitions

For this data set the positive class is "N" for enrolled_spring. This is
the event of interest for prediction. The negative category is "Y" for
did not enroll spring.

<br/><br/>







**Terms**

To evaluate model performance on predicting the True Positive class
("N") multiple plots and metrics will be used.

**TP** - True Positive. The model predicted positive and it's true.

**TN** - True Negative. The model predicted negative and it's true.

**FP** - False Positive. The model predicted positive and it's false.

**FN** - False Negative. The model predicted negative and it's false.

**ROC** - Probability curve that plots the TPR against FPR at various
threshold values.

**Accuracy** - Proportion of true results among the total number of
cases.

**Sensitivity** - Proportion of positive classes that are correctly
classified. Also known as *Total Positive Rate(TPR)* and *Recall.*

**Specificity** - Proportion of actual negatives that are correctly
classified.

**FPR** - Fall Positive Rate. Proportion of negative classes that are
incorrectly classified.

<br/><br/>










<!--#  Metrics will be collect for all models.  Individual metrics will be viewed as a visualization or in table form.   -->




```{r}


levels(ug_v10[["enrolled_spring"]])



```








```{r}



log_metrics <- log_rs %>% 
  collect_metrics(summarise = TRUE) %>%
  mutate(model = "Logistic Regression") # add the name of the model to every row




```









```{r}



rf_metrics <- rf_rs %>% 
  collect_metrics(summarise = TRUE) %>%
  mutate(model = "Random Forest")




```











```{r}


tree_metrics <- tree_rs %>% 
  collect_metrics(summarise = TRUE) %>%
  mutate(model = "Decision Tree")





```










```{r}




knn_metrics <-  knn_rs %>% 
  collect_metrics(summarise = TRUE) %>%
  mutate(model = "KNN")




```










```{r}




xgb_metrics <-  xgb_rs %>% 
  collect_metrics(summarise = TRUE) %>%
  mutate(model = "Xgbbost")





```





### Area Under the Curve Score






<!--#  Metrics will be combined  using bind_rows.   -->



```{r}




model_compare <- bind_rows(
                          log_metrics,
                           rf_metrics,
                           tree_metrics,
                            knn_metrics,
                          xgb_metrics
                          
                          
                           ) 



```










```{r}





model_comp <-  model_compare %>% 
  select(model, .metric, mean, std_err) %>% 
  pivot_wider(names_from = .metric, values_from = c(mean, std_err)) 



```








<!--#  show mean area under the curve (auc) per model  -->



```{r}








train_mod_plot<-model_comp %>% 
  arrange(mean_roc_auc) %>% 
  mutate(model = fct_reorder(model, mean_roc_auc)) %>%
  ggplot(aes(model, mean_roc_auc, fill=model)) +
  geom_col() +
  coord_flip() +
  scale_fill_brewer(palette = "Blues") + 
     geom_text(
     size = 3,
     aes(label = round(mean_roc_auc, 3), y = mean_roc_auc + 0.08),
     vjust = 1
  )




```




<br/><br/>



**Models Ranked-Area Under the Curve**




```{r}




train_mod_plot



```


<br/><br/>







*Figure 22* displays the trained models ranked by the Area Under the
Curve (AUC) score. AUC is the of the ability of a classifier to
distinguish between classes. The higher the AUC the better the
performance of the model at distinguishing between the positive and
negative classes.

An AUC score of 1 means the classifier is able to perfectly distinguish
between all the Positive and Negative classes.

If the AUC is 0, then the classifier would predict all classes
incorrectly.

When the AUC= 0.5 the the classifier is no better than predicting
classes by guessing randomly (flip of a coin).

The AUC is essentially a summery of a ROC curve which will be displayed
in *Figure 23*.

From *Figure 22* we can see that the Random Forest model has the highest
AUC score of .83. Following closely are the Logistic Regression and
XGboost model's with 0.81 and 0.80 AUC scores respectively. The AUC
score in *Figure 22* is the average of the AUC scores for the five folds
the models were trained on. Though these AUC scores for the three top
models a good (roughly a grade a B), remember we are interested how are
they at predicting the event of interest (did not attend spring) . To
gauge this effectiveness more metrics will need to be observed in orfder
to answer this question.

<br/><br/>

The next evaluation method is a ROC curve. Before reviewing the ROC
curves of each of our models we'll review what a ROC curve is and how to
understand it.

A ROC curve is the display of sensitivity(TPR) and FPR (1-Specificity)
for different cut-off values for probability.

In the *ROC Curve Example* below, the red dashed line indicates the
points where TPR and FPR are equal at any point on the line. This
suggests that the probability of correctly classified Positives is the
same as the probability of incorrectly classified Negatives which is
only as good as a ransom classifier (AUC score of 0.50).

<br/><br/>

![ROC CURVE
Example](C:%5CUsers%5Cfvlau%5COneDrive%5CDocuments%5CCambridge%20First%20Year%5Cimages%5CROC_exampl_3.png)

<br/><br/>

If the probability of positive response is above the red dashed line
(chance), the prediction is a positive outcome and is a good model. If
the probability of positive response is below the red line then the
prediction is a negative one and the model would be considered bad.

<br/><br/>

An optimal model would have sensitivity as 1.0 and FPR as 0. This means
the model is able to perfectly distinguish between positive and negative
classes. This can be visualized again with the *ROC Curve Example.* The
the blue dot in the upper left corner of the ROC Curve has a TPR of 1.0
and a FPR of 0.0. This translates into the model prediction having no
false positives and all true positives.

<br/><br/>

Optimal models are a rarity. What usually occurs is a
model that has different thresh-holds of TPR and FPR along its ROC
curve. Basically a threshold is checking what x-axis and y-axis values
are at a specific point along a diagonal line. Moving up or down along
this diagonal line will usually increase one axis while decreasing another
axis. This is how we will judge how  well a model predicts the event of
interest. Going to the *ROC Curve* example once more,  we see three curves
of different colors which represent different models. We'd like to see a
models ROC curve bend or shoulder occur as close to the upper left
corner as possible. The blue ROC curve bends at a TPR of 0.88 and a FPR
of 0.12 meaning the model predicts true positives 88% of the time while
only predicting false positives 12% of the time. This model could be
considered very good. The model represented by the green line has a
shoulder occurring roughly around a TPR of 0.72 and a FPR of 0.28. This
model could be considered above average. The third model represented by
a orange ROC curve barely has a shoulder around a TPR of 0.58 and a FPR
of 0.2. This model is not much better than average in distinguishing
between true positives and false positives. As seen in Example 2, the
blue ROC curve has a shoulder closest to the upper left corner and thus
is the better model. Different points further to the right along each of
the ROC curves brings a higher TPR along with a higher FPR.







```{r}




tree_roc<-tree_rs %>% 
  collect_predictions() %>% 
  group_by(id) %>% 
  roc_curve(enrolled_spring, .pred_N) %>% 
  ggplot(aes(1-specificity, sensitivity, color=id))+
  geom_abline(lty=2, color="gray80", size=1.5)+
  geom_path(show.legend=FALSE, alpha=0.6, size=1.2)+
  coord_equal()



```










```{r}



rf_roc<-rf_rs %>% 
  collect_predictions() %>% 
  group_by(id) %>% 
  roc_curve(enrolled_spring, .pred_N) %>% 
  ggplot(aes(1-specificity, sensitivity, color=id))+
  geom_abline(lty=2, color="gray80", size=1.5)+
  geom_path(show.legend=FALSE, alpha=0.6, size=1.2)+
  coord_equal()



```













```{r}



log_roc<-log_rs %>% 
  collect_predictions() %>% 
  group_by(id) %>% 
  roc_curve(enrolled_spring, .pred_N) %>% 
  ggplot(aes(1-specificity, sensitivity, color=id))+
  geom_abline(lty=2, color="gray80", size=1.5)+
  geom_path(show.legend=FALSE, alpha=0.6, size=1.2)+
  coord_equal()










```










```{r}




knn_roc<-knn_rs %>% 
  collect_predictions() %>% 
  group_by(id) %>% 
  roc_curve(enrolled_spring, .pred_N) %>% 
  ggplot(aes(1-specificity, sensitivity, color=id))+
  geom_abline(lty=2, color="gray80", size=1.5)+
  geom_path(show.legend=FALSE, alpha=0.6, size=1.2)+
  coord_equal()



```











```{r}




boost_roc<-xgb_rs %>% 
  collect_predictions() %>% 
  group_by(id) %>% 
  roc_curve(enrolled_spring, .pred_N) %>% 
  ggplot(aes(1-specificity, sensitivity, color=id))+
  geom_abline(lty=2, color="gray80", size=1.5)+
  geom_path(show.legend=FALSE, alpha=0.6, size=1.2)+
  coord_equal()





```












```{r}




library(ggpubr)

theme_set(theme_pubr())





```







```{r}





roc_curve<-ggarrange(log_roc,rf_roc,tree_roc,boost_roc,tree_roc,
                  labels=c("Log Reg", "RF",
                  "Tree", "XGBoost",
                  "KNN"),ncol=2, nrow=3)


```






```{r echo=FALSE, fig.cap="Figure 23 ROC Curve- All Models",fig.height=8, fig.width=8}


roc_curve






```







<br/><br/>

Figure 26 includes the ROC curves of the five folds for each model.
Viewing the five folds will help understand how the AUC score was
determined. For the Random Forest model, two of the five folds have
shoulders that are close together around a TPR range of 0.76 to 0.85 and
a FPR of 0.20. The shoulders for the other three fold's occurs around a
TPR of 0.70 and FPR of 0.30. The Logistic Regression model's ROC curve
has two folds shoulders near a TPR of 0.80 and a FPR of 0.20 while the
other three folds have a TPR range between 0.60 and 0.70 . All models
appear to have ROC curves indicating very good to good at low false
positives. Individual metrics will be analyzed to seek additional
insights.







### Heatmap


<!--#  Create heatmaps for each  model  -->





```{r}





log_htmap<-log_rs %>% 
  unnest(.predictions) %>% 
  conf_mat(enrolled_spring, .pred_class) %>% 
  autoplot(type="heatmap")




```







```{r}



rf_htmap<-rf_rs %>% 
  unnest(.predictions) %>% 
  conf_mat(enrolled_spring, .pred_class) %>% 
  autoplot(type="heatmap")




```






```{r}




tree_htmap<-tree_rs %>% 
  unnest(.predictions) %>% 
  conf_mat(enrolled_spring, .pred_class) %>% 
  autoplot(type="heatmap")




```









```{r}



boost_htmap<-xgb_rs %>% 
  unnest(.predictions) %>% 
  conf_mat(enrolled_spring, .pred_class) %>% 
  autoplot(type="heatmap")






```









```{r}





knn_htmap<-knn_rs %>% 
  unnest(.predictions) %>% 
  conf_mat(enrolled_spring, .pred_class) %>% 
  autoplot(type="heatmap")







```










```{r}




htmap_all<-ggarrange(log_htmap,rf_htmap,tree_htmap,boost_htmap,knn_htmap,
                  labels=c("Log_reg", "RF",
                  "Tree", "XGBoost",
                  "Knn"),ncol=2, nrow=3)





```











```{r echo=FALSE, fig.cap="Figure 24: Heatmap-All Models",fig.height=14, fig.width=8}





htmap_all





```





### Metrics





<!--#  Transform model_compare into data frame  -->





```{r}


model_compare_df<-as.data.frame(model_compare)




```







<!--#  Extract accuracy metric from  data frame  -->


```{r}





model_comp_accuracy<-model_compare_df %>% 
  select(-n, -.estimator, -std_err,-.config) %>% 
  filter(.metric=="accuracy") %>% 
  arrange(desc(mean))



```







```{r}





accuracy_table <- ggtexttable(model_comp_accuracy, rows = NULL, 
                        theme = ttheme("mBlue", base_size=14)) %>% 

  tab_add_title("Accuracy Scores-Trained Data",face="bold", size=10) %>% 
  tab_add_footnote("Table 12", size=9)




```








```{r}



accuracy_table




```









<br/><br/>

<br/><br/>

*Table 12* contains the accuracy score for all models. The accuracy
score is calculated by dividing the total of true predictions ( N and Y
were predicted accurately) by the total number of cases. Accuracy is not
always the best metric for determining a models performance. A models
accuracy score can be inflated by it being overwhelming good at
predicting one class while on the other hand being only average at
predicting the other class. We'll use the XGboost models as an example.
Based on *Table 12* the XGBoost model has the accuracy score of .79,
second only to the .80 score of the Random Forest model. *Figure 27*
includes confusion matrix's as heat maps for all models. From it we can
we can gather all true predictions. True predictions are viewed in the
right diagonal in which TP is located at the upper left quadrant and TN
is located at the lower right quadrant. XGboost's TP is the lowest of
all models. From this we can discern that the XGBoost model is very good
at predicting the negative class and not as good at predicting the
positive class.

<br/><br/>






<!--#  Extract sensitivity metric from  data frame  -->







```{r}



model_comp_sensitivity<-model_compare_df %>% 
  select(-n, -.estimator, -std_err,-.config) %>% 
  filter(.metric=="sensitivity") %>% 
  arrange(desc(mean))




```











```{r}


sensitivity_table <- ggtexttable(model_comp_sensitivity, rows = NULL, 
                        theme = ttheme("mOrange", base_size=14)) %>% 
   tab_add_title("Sensitivity Scores-Trained Data",face="bold", size=10) %>% 
  tab_add_footnote("Table 13", size=9)




```










```{r}



sensitivity_table


```





<br/><br/>

*Table 13* exhibits the sensitivity scores for all models.

<br/><br/>







<!--#  Extract specificity metric from  data frame  -->






```{r}


model_comp_specificity<-model_compare_df %>% 
  select(-n, -.estimator, -std_err,-.config) %>% 
  filter(.metric=="specificity") %>% 
  arrange(desc(mean))





```










```{r}




specificity_table <- ggtexttable(model_comp_specificity, rows = NULL, 
                        theme = ttheme("mGreen", base_size=14)) %>% 
   tab_add_title("Specificity Scores-Trained Data",face="bold", size=10) %>% 
  tab_add_footnote("Table 14",size=9)




```




<br/><br/>

*Table 13* and *Table 14* provide a clearer picture of how the models
performed on the predicting the positive class (did not enroll for
spring term)

*Table 13* provides us with the sensitivity score for all models. This
score gives us an idea about when the positive class is yes and how
often does it predict yes. The formula is TP/(TP+FN). The logistic
regression model has the highest score of 0.65. This means for the
logistic regression model just over 4 out of every 10 students in realty
are miss-labeled as enrolling for the spring term and just under 7 are
correctly labeled as not enrolling for the spring, The next closest
models are the Random Forest and decision tree both with a score of .60. The
XGboost model predicts the positive class no better than 50-50.

*Figure 14* gives us the specificity score of all the models. This score
tells us when the negative class is predicted correctly (Students
enrolled for the spring. From this figure it appears all models good at
predicting the negative class. The Random Forest and Xgboost models have
scores of 0.88 and 0.87 placing them at very good at predicting the
negative class. This means that just over 1 out of every 10 students in
reality are miss-labeled as not enrolling for the spring term and just
under 9 correctly labeled as enrolling for the spring term. XGboost's
high specificity score low sensitivity score supports our observation
about the inflation of its accuracy score.

<br/><br/>

<br/><br/>

## Model Performance-Test Set (unseen data)

<br/><br/>

Models will now be evaluated on how well they perform on the test set.
It's important to determine how well a model performs on unseen data as
it is an indicator on well it performs on outside data. We will observe
whether a model is over fit or under fit. Over fitting is when a model's
performance is higher on the training set than on the test set. This
could mean that the model is fit closely with the specifics of the
training set yet can not replicate this performance on the test set.
This might occur due to to many features. Under fitting is when a models
performance is lower on the training set than on the test set. This
could mean more features are required for the fitting of the model.

<br/><br/>




```{r}



specificity_table




```










<br/><br/>

*Table 13* and *Table 14* provide a clearer picture of how the models
performed on the predicting the positive class (did not enroll for
spring term)

*Table 13* provides us with the sensitivity score for all models. This
score gives us an idea about when the positive class is yes and how
often does it predict yes. The formula is TP/(TP+FN). The logistic
regression model has the highest score of 0.65. This means for the
logistic regression model just over 4 out of every 10 students in realty
are miss-labeled as enrolling for the spring term and just under 7 are
correctly labeled as not enrolling for the spring, The next closest
models are the Random Forest and decision tree with a score of .60. The
XGboost he model predicts the positive class no better than 50-50.

*Figure 14* gives us the specificity score of all the models. This score
tells us when the negative class is predicted correctly (Students
enrolled for the spring. From this figure it appears all models good at
predicting the negative class. The Random Forest and Xgboost models have
scores of 0.88 and 0.87 placing them at very good at predicting the
negative class. This means that just over 1 out of every 10 students in
reality are miss-labeled as not enrolling for the spring term and just
under 9 correctly labeled as enrolling for the spring term. XGboost's
high specificity score low sensitivity score supports our observation
about the inflation of its accuracy score.

<br/><br/>

<br/><br/>

## Model Performance-Test Set (unseen data)

<br/><br/>

Models will now be evaluated on how well they perform on the test set.
It's important to determine how well a model performs on unseen data as
it is an indicator on how well it performs on outside data. We will observe
whether a model is over fit or under fit. Over fitting is when a model's
performance is higher on the training set than on the test set. This
could mean that the model is fit closely with the specifics of the
training set yet can not replicate this performance on the test set.
This might occur possibly due too many features. Under fitting is when a model's
performance is lower on the training set than on the test set. This
could mean more features are required for the fitting of the model.

<br/><br/>














<!--#  The last_fit function from tidy models will pull the test data from the new_ug_splt data frame.  -->




<!--#  The function collect_metrics will  pull the metrics for the test set (last fit)  -->


```{r}



ug_final_log<-logreg_wf %>% 
  last_fit(new_ug_split)


```











```{r}




final_metrics_logreg<-collect_metrics(ug_final_log)

final_metrics_logreg<-final_metrics_logreg %>% 
  select(.metric, .estimate) %>% 
   pivot_wider(names_from = .metric, values_from = c(.estimate)) 





```










```{r}



model<-"log_reg"

final_metrics_logreg$model<-model





```














```{r}



ug_final_tree<-tree_wf %>% 
  last_fit(new_ug_split)




```







```{r}




final_metrics_dectree<-collect_metrics(ug_final_tree)


final_metrics_dectree<-final_metrics_dectree %>% 
  select(.metric, .estimate) %>% 
   pivot_wider(names_from = .metric, values_from = c(.estimate)) 




```










```{r}



model<-"decision tree"

final_metrics_dectree$model<-model




```









```{r}



ug_final_rf<-rf_wf %>% 
  last_fit(new_ug_split)




```











```{r}




final_metrics_rf<-collect_metrics(ug_final_rf)



final_metrics_rf<-final_metrics_rf %>% 
  select(.metric, .estimate) %>% 
   pivot_wider(names_from = .metric, values_from = c(.estimate)) 




```







```{r}

model<-"random forest"

final_metrics_rf$model<-model



```










```{r}



ug_final_xgb<-xgb_wf %>% 
  last_fit(new_ug_split)






```









```{r}





final_metrics_xgb<-collect_metrics(ug_final_xgb)


final_metrics_xgb<-final_metrics_xgb %>% 
  select(.metric, .estimate) %>% 
   pivot_wider(names_from = .metric, values_from = c(.estimate)) 






```











```{r}



model<-"xgboost"


final_metrics_xgb$model<-model





```









```{r}





ug_final_knn<-knn_wf %>% 
  last_fit(new_ug_split)





```











```{r}




final_metrics_knn<-collect_metrics(ug_final_knn)


final_metrics_knn<-final_metrics_knn %>% 
  select(.metric, .estimate) %>% 
   pivot_wider(names_from = .metric, values_from = c(.estimate)) 




```









```{r}




model<-"knn"

final_metrics_knn$model<-model





```








```{r}



test_metrics_combined<-rbind(final_metrics_logreg,
           final_metrics_rf, final_metrics_dectree,
           final_metrics_xgb, final_metrics_knn)





```








```{r}



test_comb_rocauc<-test_metrics_combined %>%   select(model, roc_auc) %>% 
  arrange(desc(roc_auc))








```









```{r}









test_rocauc_table <- ggtexttable(test_comb_rocauc, rows = NULL, 
                        theme = ttheme("mViolet", base_size=14)) %>% 
   tab_add_title("Roc Auc Scores-Test Data",face="bold", size=9) %>% 
  tab_add_footnote("Table 15", size=9)
```








```{r}



test_rocauc_table




```



### Accuracy








```{r}




test_comb_accuracy<-test_metrics_combined %>%   select(model, accuracy) %>% 
  arrange(desc(accuracy))





```









```{r}



test_accuracy_table <- ggtexttable(test_comb_accuracy, rows = NULL, 
                        theme = ttheme("mCyan", base_size=14)) %>% 
   tab_add_title("Accuracy Scores-Test Data",face="bold", size=9) %>% 
  tab_add_footnote("Table 15", size=9)







```







test_accuracy_table







### Heat Map

<br/><br/>











```{r}


log_final_htmap<-ug_final_log %>% 
  unnest(.predictions) %>% 
  conf_mat(enrolled_spring, .pred_class) %>% 
  autoplot(type="heatmap")









```










```{r}




log_final_htmap







```







```{r}





rf_final_htmap<-ug_final_rf %>% 
  unnest(.predictions) %>% 
  conf_mat(enrolled_spring, .pred_class) %>% 
  autoplot(type="heatmap")







```









```{r}


rf_final_htmap







```












```{r}





xgb_final_htmap<-ug_final_xgb %>% 
  unnest(.predictions) %>% 
  conf_mat(enrolled_spring, .pred_class) %>% 
  autoplot(type="heatmap")





```













```{r}




xgb_final_htmap







```










```{r}







tree_final_htmap<-ug_final_tree %>% 
  unnest(.predictions) %>% 
  conf_mat(enrolled_spring, .pred_class) %>% 
  autoplot(type="heatmap")







```










```{r}




tree_final_htmap





```









```{r}





knn_final_htmap<-ug_final_knn %>% 
  unnest(.predictions) %>% 
  conf_mat(enrolled_spring, .pred_class) %>% 
  autoplot(type="heatmap")



```












```{r}



knn_final_htmap





```










```{r}





htmap_test_all<-ggarrange(log_final_htmap,rf_final_htmap,tree_final_htmap,xgb_final_htmap,knn_final_htmap,
                  labels=c("Log_reg", "RF",
                  "Tree", "XGBoost",
                  "Knn"),ncol=2, nrow=3)






```










```{r}




htmap_test_all





```





<br/><br/>

Table 15 presents the AUC scores for the models from the test set. The
Logistic Regression model has the highest score at 0.84 followed closely
by the Random Forest model at 0.82. These scores a very close to those
from the training set thus we can conclude that both models fit well to
unseen data. The other three models have slightly lower scores than the
training set though still fit well to unseen data.

<br/><br/>

The accuracy scores from *Table 15* are close to those from the training
set with the exception of the logistic regression model which increased
by 4%. Logistic Regression was also the only model to improve on its
sensitivity score. Using data from the confusion matrix in *Figure 25*
we divide true positives by true positives plus false positives.

82/(82+32)=0.719

The sensitivity score of 0.72 for the test set is just over a four
percent increase from the training set score. This can be interpreted as
7 out of 10 students are correctly predicted as not enrolling for spring
and 3 out of 10 are incorrectly predicted as not enrolling for spring.

<br/><br/>





## Feature Analysis

<br/><br/>

Each model has features that carry more importance in the final
predictions than other features. Each model will be analyzed to find
which features had the most sway when determining the final prediction.

<br/><br/>







### Logistic Regression

<br/><br/>

For Logistic Regression we want to find the significant features
(coefficients). The p-value is the method used by logistics regression
to determine significant features. Significance can be defined as the
outcome features that is not random or by chance. The universal base
line p-value used is 0.05 though this can vary depending on field and
subject. Features with a p-value equal to or less than 0.05 are
considered significant. For our model we can check *Figure 26* to find
which features are significant. Those features colored in blue (not
crossing the horizontal line) are considered significant.

<br/><br/>

Now that we know which features are significant, how each effects the
event prediction (outcome) will be reviewed next.








```{r}


options(scipen=999)





```









<!--#  The pull()function will extract the coefficients from the final logistic regression model.  -->




```{r}





final_log_feat<-ug_final_log %>% 
  pull(.workflow) %>% 
  pluck(1) %>% 
  tidy(exponentiate=TRUE) %>% 
  arrange(estimate) 

final_log_feat$p.value<-round(final_log_feat$p.value, 3)

final_log_feat$estimate<-round(final_log_feat$estimate, 3)




```








<!--#  Use ifelse statement to divide coefficients into significant or insignificant.   -->



```{r}



final_fit<-final_log_feat %>% 
  mutate(Significance=ifelse(p.value <.051, "Significant", 
                             "Insignificant")) %>% 
  arrange(desc(p.value))







```












```{r}






log_fit_plot<-final_fit %>% 
  ggplot(aes(x=term, y=p.value, fill=Significance))+
  geom_col()+
  theme(axis.text.x=element_text(
    face="bold", color="#0070BA",
    size=6, angle=90
  ))+
  scale_y_continuous(limits=c(.00,1.0))+
  labs(y="P value", x="Terms",
       title="p value significance chart",
       subtitle="A chart to represent the sigificant variables in the model")+
  geom_hline(yintercept = .05)






```








```{r echo=FALSE, fig.cap="Figure 26"}



log_fit_plot





```







```{r}




log_table<-final_log_feat %>% 
  select(term, estimate, std.error, p.value) %>% 
  filter(p.value <0.051)




```








```{r}



log_reg_feat_table<- ggtexttable(log_table, rows = NULL, 
                        theme = ttheme("mRed", base_size=12)) %>% 
   tab_add_title("Logistic Regression Features",face="bold", size=9) %>% 
  tab_add_footnote("Table 17", size=6)

"Table 19: Logistic Regression Significant Features"





```








```{r}





log_reg_feat_table






```








<br/><br/>

*Table 17* gives us the output of the Logistic Regression model. For the
purpose of evaluation we will be using the term, estimate, and p.value
columns. The estimate is how much the term affects the event prediction.
It's output is in the form of odds ratio. Odds ratios that are greater
than 1 indicate that the event of a student returning for the spring
term is more likely to occur. Odds ratios that are less than 1 indicate
that the event of a student returning for the spring is less likely to
occur. For easier interpretation the estimate will also be given as a
percent.

<br/><br/>

-   less.than.half.time-Students enrolled as less than half time are 90%
    less likely to return for the spring term than students enrolled as
    full time keeping other variables constant.

-   half time-Students enrolled as half time are 56% less likely to
    return for the spring term than students enrolled as full time
    keeping other variables constant.

-   race.African.American-African American students are 52% less likely
    to return for the spring term than students who are Asian keeping
    other variables constant.

-   gender_M-Males students are 35% less likely to return than female
    students keeping other variables constant.

-   xfer_hrs_earned-For every one credit increase in transfer hours
    earned the odds of students returning for the spring term increase
    by 30% keeping other variables constant.

-   pct_enrolled_cr_earned-For every one percent increase in earned
    enrolled credits the odds of students returning for the spring
    increase by 39%

-   filed_fafsa_Y-Students who file a FAFSA are 76% more likely to
    return for the spring term than students who do not file a FAFSA
    keeping other variables constant.

-   trm_gpa-For every one unit increase in GPA the odds of students
    returning for the spring term increase by 166%.

<br/><br/>













```{r}




library(vip)







```




<br/><br/>

<br/><br/>

<br/><br/>





<!--#  VIP function will pull important features for tree based models.    -->




```{r}



tree_features<-ug_final_tree %>% 
  pluck(".workflow", 1) %>%   
  extract_fit_parsnip() %>% 
  vip(geom="col", aesthetics=list(fill="midnightblue", alpha = 0.8))+
  scale_y_continuous(expand=c(0,0))




```







```{r echo=FALSE, fig.cap="Figure 27: Feature Importance-Decision Tree",fig.height=9, fig.width=8}




tree_features





```











```{r}





feature_importance_rf<-ug_final_rf %>% 
  pluck(".workflow", 1) %>%   
  extract_fit_parsnip() %>% 
  vip(geom="col", aesthetics=list(fill="#d75237", alpha = 0.8))+
  scale_y_continuous(expand=c(0,0))





```










```{r echo=FALSE, fig.cap="Figure 28: Feature Importance-Random Forest",fig.height=9, fig.width=6}





feature_importance_rf




```








```{r}






feature_imortance_xgb<-ug_final_xgb %>% 
  pluck(".workflow", 1) %>%   
  extract_fit_parsnip() %>% 
  vip(geom="col", aesthetics=list(fill="#419f52", alpha = 0.8))+
  scale_y_continuous(expand=c(0,0))




```










```{r echo=FALSE,  fig.cap="Figure 29: Feature Importance-Xgboost",fig.height=9, fig.width=6}






feature_imortance_xgb




```












<br/><br/>

### Tree Based Models

<br/><br/>

The evaluation of important features is different for tree based models
compared to a logistic regression model. In the case of tree based
models, feature importance does not indicate if the effects on the
event prediction is positive or negative only that the feature had a
strong effect on the prediction.

The Decision Tree model has trm gpa and pct_enrolled_credits_earned as
the most important features.

For the Random Forest model the most important features, trm_gpa and
pct_enrolled_cr_earned, are the same as the Decision Tree model though
with less impact.

The XGBoost model has trm_gpa and pct_enrolled_cr_earned, the same as
the first two models and in addition, also has the xfer_hrs_earned, age, and
accpt_Start_diff as impact-full features. Unlike The Decision Tree and
Random Forest models, XGBoost has more features that impact the event
prediction though not as dominating.

<br/><br/>






# Conclusion

<br/><br/>

<br/><br/>





Based on the model performance evaluation we can conclude that the
logistic regression model had the  best performance. Best though does not necessarily
mean it is a good model. Remember that the event of interest was if a
student did not return for the spring term. Yes, the Logistic Regression
model was by far the best at predicting the event of interest, yet it was
only 70% accurate in predicting a student not returning. Improving model
performance may require more data and finding more features that can be
used in the fitting of the model.



<br/><br/>



Still,  the models did provide insights on the first time undergraduate
population. Between the significant features from the logistic
regression model, the tree based feature importance, and the data
visualizations,  there's enough data to develop an action plan that focuses
on at risk students.














```{r}










```

